{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ì‹¤ìŠµ ëª©í‘œ**\n",
        "**í•˜ì´í¼íŒŒë¼ë¯¸í„°**ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì—, ì ì ˆí•œ ê°’ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.   \n",
        "**Ray**ëŠ” ì´ëŸ° **í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ìµœì ê°’ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì°¾ì•„ë‚´ê¸° ìœ„í•œ ë„êµ¬**ë¡œ, ë³‘ë ¬ ì—°ì‚° ë° ë‹¤ì–‘í•œ ìµœì í™” ì „ëµì„ ì œê³µí•œë‹¤. ì´ë¥¼ í†µí•´ í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¤ê³  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™” í•  ìˆ˜ ìˆë‹¤.     \n",
        "CIFAR-10 ë°ì´í„°ì…‹ì„ í™œìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì— Rayë¥¼ PyTorchì™€ ì—°ë™í•˜ì—¬ í•™ìŠµì‹œì¼œë³´ì."
      ],
      "metadata": {
        "id": "J2d5yj5Rr7DF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ëª¨ë¸ì´ ì„±ëŠ¥ì´ ì•ˆ ë‚˜ì˜¬ ê²½ìš°**\n",
        "1. ëª¨ë¸ì„ ë°”ê¿”ë³´ê¸°\n",
        "    - ì‚¬ì‹¤ìƒ ê°€ì¥ ë§ì€ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ë§Œ, ì´ë¯¸ ì¢‹ì€ ëª¨ë¸ì´ ë§ì´ ë‚˜ì™€ìˆìŒ.\n",
        "2. Data ë°”ê¿”ë³´ê¸°(ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜, ê¸°ì¡´ dataì— ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸)\n",
        "    - ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„.\n",
        "    - DataëŠ” ë§ìœ¼ë©´ ë§ì„ ìˆ˜ë¡ ì¢‹ìŒ\n",
        "3. Hyperparameter Tunning\n",
        "    - ê·¸ë ‡ê²Œ ì˜í–¥ì´ í¬ì§„ ì•ŠìŒ.\n",
        "    - ë§ˆì§€ë§‰ì˜ ë§ˆì§€ë§‰ ë°©ë²•ì¸ ëŠë‚Œ\n",
        "    - ì¤‘ìš”ì„±ì€ ë‚®ì•„ì¡Œì§€ë§Œ ê·¸ë˜ë„ í•¨."
      ],
      "metadata": {
        "id": "O-x9IhrfdmIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**â“í€´ì¦ˆ**     \n",
        "### **Hyperparameter**\n",
        "- **ğŸ–Š ì •ë‹µ:** ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ì§€ ì•ŠëŠ” ê°’\n",
        "    - ì‚¬ëŒì´ ì§€ì •í•´ì¤˜ì•¼ í•¨\n",
        "    - learning rate, ëª¨ë¸ì˜ í¬ê¸°, optimizer ë“±\n",
        "    "
      ],
      "metadata": {
        "id": "p1_IJsRWeWia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Grid Layout & Random Layout**\n",
        "ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•. ìµœê·¼ì—ëŠ” ë² ì´ì§€ì•ˆ ê¸°ë°˜ ê¸°ë²•ë“¤ì´ ì£¼ë„í•˜ê³  ìˆìŒ.    \n",
        "<br>\n",
        "\n",
        "**â“í€´ì¦ˆ**     \n",
        "#### **Grid Layout**\n",
        "- ì¼ì •í•œ ë²”ìœ„ë¥¼ ì •í•´ì„œ ê°’ì„ ìë¦„\n",
        "- **ğŸ–Š ì •ë‹µ:** í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•©ì„ ì‹œí—˜í•˜ì—¬ ìµœì ì˜ ì¡°í•©ì„ ì°¾ëŠ” ë°©ë²•\n",
        "- í•˜ë‚˜ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ê³¨ë¼ì„œ í•™ìŠµ ìˆ˜í–‰, ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì„ ì°¾ìŒ.\n",
        "- lrëŠ” ë¡œê·¸ë¥¼ ì·¨í•´ì„œ ì‚¬ìš©\n",
        "\n",
        "#### **Random Layout**\n",
        "ëœë¤í•˜ê²Œ ê°’ì„ ì°¾ì•„ í•™ìŠµì„ ìˆ˜í–‰, ê·¸ ì¤‘ ê°€ì¥ ì˜ ë‚˜ì˜¨ ê°’ì„ ì‚¬ìš©\n"
      ],
      "metadata": {
        "id": "351RHWUme6JA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ray**\n",
        "- Hyperparameter tunningì˜ ëŒ€í‘œì ì¸ ë„êµ¬\n",
        "- multi node multi processing ì§€ì› ëª¨ë“ˆ\n",
        "- ML/DLì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê°œë°œ\n",
        "- Hyperparameter Searchë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ëª¨ë“ˆ ì œê³µ\n",
        "<br><br>\n",
        "\n",
        "**â“í€´ì¦ˆ:** Rayì— ê´€í•œ ì„¤ëª… ì¤‘ ê°€ì¥ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²ƒì€?       \n",
        "**ğŸ–Š ì •ë‹µ:** Hyperparameterì—ë§Œ íŠ¹í™”ë˜ì–´ìˆë‹¤."
      ],
      "metadata": {
        "id": "XfJxxwEEfzSm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C_Q9oeXCGIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d985a3-0b71-441c-99f6-d52f22c6d109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (2.24.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.14.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.31.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ray"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RayëŠ” ë‚´ë¶€ì ìœ¼ë¡œ tensorboardXë¼ëŠ” ëª¨ë“ˆì„ ì‚¬ìš©í•¨\n",
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "D2fPvAX2CLA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112c46ee-e176-40eb-8d86-7cf3ad1a3df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "fOcxEv6zCP5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac2c388-487a-4d03-ff14-aafa1bcab03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.5.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬**"
      ],
      "metadata": {
        "id": "FRhkhFIyCWS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ray\n",
        "import wandb\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from functools import partial\n",
        "from torch.utils.data import random_split\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.search.bayesopt import BayesOptSearch\n",
        "from ray.tune.search.hyperopt import HyperOptSearch"
      ],
      "metadata": {
        "id": "nFB-4-5eCTfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir='./data'):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    # CIFAR10 ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    return trainset, testset"
      ],
      "metadata": {
        "id": "iGc-tGYDCadU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜**"
      ],
      "metadata": {
        "id": "L2zxIbnIDRou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    # l1, l2ëŠ” ì—¬ê¸°ì„œëŠ” ë§ˆì§€ë§‰ layerë“¤ì˜ í¬ê¸°ì— ê´€ë ¨ëœ íŒŒë¼ë¯¸í„°\n",
        "    def __init__(self, l1=120, l2=84):\n",
        "        super(Net, self).__init__()\n",
        "        # ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì™€ í’€ë§ ë ˆì´ì–´\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 10)\n",
        "\n",
        "    # ìˆœì „íŒŒ ì •ì˜\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "bzaOXIutDXR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ëª¨ë¸ í•™ìŠµ**"
      ],
      "metadata": {
        "id": "Zd4JshMIGwCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ê³¼ì •ì˜ ì²˜ìŒë¶€í„° ëê¹Œì§€ í•˜ë‚˜ì˜ í•¨ìˆ˜ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼, Rayê°€ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤\n",
        "def train_cifar(config, data_dir=None):\n",
        "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    net = Net(config[\"l1\"], config[\"l2\"])\n",
        "\n",
        "    # ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜ í™•ì¸ (GPU ë˜ëŠ” CPU)\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)  # ë©€í‹° GPU ì‚¬ìš© ì‹œ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬\n",
        "    net.to(device)  # ëª¨ë¸ì„ í•´ë‹¹ ì¥ì¹˜ë¡œ ì´ë™\n",
        "\n",
        "    # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜: êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ ì‚¬ìš©\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜: SGD ì‚¬ìš©\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
        "\n",
        "    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë° ìµœì í™” ìƒíƒœ ë¡œë“œ (ìˆì„ ê²½ìš°)\n",
        "    checkpoint = ray.train.get_checkpoint()\n",
        "    if checkpoint:\n",
        "        model_state, optimizer_state = torch.load(checkpoint.path)\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    # ë°ì´í„° ë¡œë“œ ë° í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
        "    trainset, testset = load_data(data_dir)\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs])\n",
        "\n",
        "    # ë°ì´í„° ë¡œë” ì„¤ì •\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "\n",
        "    # wandbë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§\n",
        "    wandb.init(project='torch-turn', entity='nayoungpark')\n",
        "    wandb.watch(net)\n",
        "\n",
        "    # í•™ìŠµ ì‹œì‘\n",
        "    for epoch in range(10):  # ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ 10ë²ˆ ë°˜ë³µ\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # ì…ë ¥ ë°ì´í„° ë° ë ˆì´ë¸” ë¡œë“œ\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ì†ì‹¤ ê°’ ëˆ„ì \n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "\n",
        "            # 2000 ë¯¸ë‹ˆ ë°°ì¹˜ë§ˆë‹¤ ì†ì‹¤ ì¶œë ¥\n",
        "            if i % 2000 == 1999:\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ ê³„ì‚°\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "\n",
        "        # wandbì— í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤ ë¡œê¹…\n",
        "        wandb.log({\"val_loss\": val_loss})\n",
        "        wandb.log({\"loss\": loss})\n",
        "\n",
        "        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "        # ì´ ë¶€ë¶„ì€ ë²„ì „ ì´ìŠˆë¡œ ì¸í•´ ê°•ì˜ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "        checkpoint = ray.train.get_checkpoint()\n",
        "        path = checkpoint.save_path(\"checkpoint\")\n",
        "        torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        # Ray Tuneì— ì†ì‹¤ ë° ì •í™•ë„ ë³´ê³ \n",
        "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "\n",
        "    print(\"Finished Training\")"
      ],
      "metadata": {
        "id": "UbRXqPCyGykg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€**"
      ],
      "metadata": {
        "id": "2Dx6nom0lYjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(net, device='cpu'):\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ì„¤ì •\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset,\n",
        "        batch_size=4,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # ì •í™•í•˜ê²Œ ë¶„ë¥˜ëœ ì´ë¯¸ì§€ ìˆ˜ ì´ˆê¸°í™”\n",
        "    correct = 0\n",
        "\n",
        "    # ì „ì²´ ì´ë¯¸ì§€ ìˆ˜ ì´ˆê¸°í™”\n",
        "    total = 0\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ì¤‘ì—ëŠ” ì—­ì „íŒŒê°€ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ë¹„í™œì„±í™”\n",
        "    with torch.no_grad():\n",
        "        for data in testloader: # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ íƒ\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # ì „ì²´ ì´ë¯¸ì§€ ìˆ˜ ì—…ë°ì´íŠ¸\n",
        "            total += labels.size(0) # labelsì˜ ì²«ë²ˆì§¸ ì°¨ì› ë°˜í™˜. ë°°ì¹˜ í¬ê¸°\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct/total"
      ],
      "metadata": {
        "id": "uyAuIgXJlbTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. ë©”ì¸ ì‹¤í–‰**"
      ],
      "metadata": {
        "id": "yFp3OJCsm1rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    # ë°ì´í„° ë””ë ‰í† ë¦¬ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "\n",
        "    # ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ í˜¸ì¶œ\n",
        "    load_data(data_dir)\n",
        "\n",
        "    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "    config = {# configì— search space ì§€ì •\n",
        "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "    }\n",
        "\n",
        "    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ASHA ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©)\n",
        "    # ë² ì´ì‹œì•ˆ optimizationê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤.\n",
        "    scheduler = ASHAScheduler( # ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì¤‘ê°„ì— ì˜ë¯¸ì—†ë‹¤ê³  ìƒê°í•˜ëŠ” lossê°’ì´ ì˜ ì•ˆë‚˜ì˜¤ëŠ” metricë“¤ì„ ì˜ë¼ë‚´ëŠ” ì•Œê³ ë¦¬ì¦˜.\n",
        "                               # ì „ì²´ë¥¼ ê°€ì§€ê³  íŠœë‹ì„ í•˜ë©´ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ê³  ì•ˆ ì“°ëŠ” ê²°ê³¼ë“¤ì´ ë‚˜ì˜¨ë‹¤.\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "\n",
        "    # ë¦¬í¬í„° ì„¤ì •\n",
        "    reporter = CLIReporter(# Command line ì¶œë ¥ ë°©ì‹ ì§€ì •\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "\n",
        "    # Ray Tuneì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì‹¤í–‰\n",
        "    result = tune.run( # ë³‘ë ¬ ì²˜ë¦¬ ì–‘ì‹\n",
        "        partial(train_cifar, data_dir=data_dir), # partial: ë°ì´í„°ë¥¼ ìª¼ê°œëŠ” í•¨ìˆ˜\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter) # ì—¬ëŸ¬ê°œì˜ GPUì— ë¿Œë ¤ì ¸ì„œ í•™ìŠµ ì§„\n",
        "\n",
        "    # ìµœì ì˜ íŠ¸ë¼ì´ì–¼ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    # ìµœì ì˜ íŠ¸ë¼ì´ì–¼ë¡œ ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    # ìµœì ì˜ íŠ¸ë¼ì´ì–¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì •í™•ë„ ê³„ì‚°\n",
        "    test_acc = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # WandB ë¡œê·¸ì¸ ë° ë©”ì¸ í•¨ìˆ˜ í˜¸ì¶œ\n",
        "    wandb.login(key=\"\")\n",
        "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tCL8KsHHm5EN",
        "outputId": "b30034ed-4b3d-4383-8ad3-923a3fc63969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhcc9876\u001b[0m (\u001b[33mnayoungpark\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-17 02:55:07,651\tINFO worker.py:1753 -- Started a local Ray instance.\n",
            "2024-06-17 02:55:09,539\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
            "2024-06-17 02:55:10,369\tWARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------+\n",
            "| Configuration for experiment     train_cifar_2024-06-17_02-55-09   |\n",
            "+--------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator             |\n",
            "| Scheduler                        AsyncHyperBandScheduler           |\n",
            "| Number of trials                 10                                |\n",
            "+--------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/train_cifar_2024-06-17_02-55-09\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts`\n",
            "\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2024-06-17 02:55:11. Total running time: 1s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00000   PENDING    0.0468533               16 |\n",
            "| train_cifar_0289e_00001   PENDING    0.0407774                8 |\n",
            "| train_cifar_0289e_00002   PENDING    0.0111418                2 |\n",
            "| train_cifar_0289e_00003   PENDING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "+-----------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_0289e_00000 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_0289e_00000 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                    16 |\n",
            "| l1                                            16 |\n",
            "| l2                                            32 |\n",
            "| lr                                       0.04685 |\n",
            "+--------------------------------------------------+\n",
            "\u001b[36m(func pid=5938)\u001b[0m Files already downloaded and verified\n",
            "\u001b[36m(func pid=5938)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=5938)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=5938)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[36m(func pid=5938)\u001b[0m wandb: Currently logged in as: hcc9876 (nayoungpark). Use `wandb login --relogin` to force relogin\n",
            "\u001b[36m(func pid=5938)\u001b[0m wandb: Tracking run with wandb version 0.17.1\n",
            "\u001b[36m(func pid=5938)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/working_dirs/train_cifar_0289e_00000_0_batch_size=16,lr=0.0469_2024-06-17_02-55-10/wandb/run-20240617_025522-ka506r1c\n",
            "\u001b[36m(func pid=5938)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[36m(func pid=5938)\u001b[0m wandb: Syncing run eager-jazz-29\n",
            "\u001b[36m(func pid=5938)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/nayoungpark/torch-turn\n",
            "\u001b[36m(func pid=5938)\u001b[0m wandb: ğŸš€ View run at https://wandb.ai/nayoungpark/torch-turn/runs/ka506r1c\n",
            "\u001b[36m(func pid=5938)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=5938)\u001b[0m   warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2024-06-17 02:55:42. Total running time: 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00000   RUNNING    0.0468533               16 |\n",
            "| train_cifar_0289e_00001   PENDING    0.0407774                8 |\n",
            "| train_cifar_0289e_00002   PENDING    0.0111418                2 |\n",
            "| train_cifar_0289e_00003   PENDING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5938)\u001b[0m [1,  2000] loss: 2.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-17 02:55:54,780\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_0289e_00000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2613, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5938, ip=172.28.0.12, actor_id=ec66c376341c83d5f8945fe901000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 98, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-7-fbecd79468fb>\", line 98, in train_cifar\n",
            "AttributeError: 'NoneType' object has no attribute 'save_path'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_0289e_00000 errored after 0 iterations at 2024-06-17 02:55:54. Total running time: 44s\n",
            "Error file: /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00000_0_batch_size=16,lr=0.0469_2024-06-17_02-55-10/error.txt\n",
            "\n",
            "Trial train_cifar_0289e_00001 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_0289e_00001 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                     8 |\n",
            "| l1                                             4 |\n",
            "| l2                                           256 |\n",
            "| lr                                       0.04078 |\n",
            "+--------------------------------------------------+\n",
            "\u001b[36m(func pid=6298)\u001b[0m Files already downloaded and verified\n",
            "\u001b[36m(func pid=6298)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=6298)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=6298)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[36m(func pid=6298)\u001b[0m wandb: Currently logged in as: hcc9876 (nayoungpark). Use `wandb login --relogin` to force relogin\n",
            "\u001b[36m(func pid=6298)\u001b[0m wandb: Tracking run with wandb version 0.17.1\n",
            "\u001b[36m(func pid=6298)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/working_dirs/train_cifar_0289e_00001_1_batch_size=8,lr=0.0408_2024-06-17_02-55-11/wandb/run-20240617_025603-h3o2bocd\n",
            "\u001b[36m(func pid=6298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[36m(func pid=6298)\u001b[0m wandb: Syncing run solar-deluge-30\n",
            "\u001b[36m(func pid=6298)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/nayoungpark/torch-turn\n",
            "\u001b[36m(func pid=6298)\u001b[0m wandb: ğŸš€ View run at https://wandb.ai/nayoungpark/torch-turn/runs/h3o2bocd\n",
            "\u001b[36m(func pid=6298)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=6298)\u001b[0m   warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 ERROR | 1 RUNNING | 8 PENDING\n",
            "Current time: 2024-06-17 02:56:12. Total running time: 1min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00001   RUNNING    0.0407774                8 |\n",
            "| train_cifar_0289e_00002   PENDING    0.0111418                2 |\n",
            "| train_cifar_0289e_00003   PENDING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=6298)\u001b[0m [1,  2000] loss: 2.318\n",
            "\u001b[36m(func pid=6298)\u001b[0m [1,  4000] loss: 1.157\n",
            "Trial status: 1 ERROR | 1 RUNNING | 8 PENDING\n",
            "Current time: 2024-06-17 02:56:42. Total running time: 1min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00001   RUNNING    0.0407774                8 |\n",
            "| train_cifar_0289e_00002   PENDING    0.0111418                2 |\n",
            "| train_cifar_0289e_00003   PENDING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "+-----------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-17 02:56:52,639\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_0289e_00001\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2613, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=6298, ip=172.28.0.12, actor_id=b6105bebe875d9f73c30735401000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 98, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-7-fbecd79468fb>\", line 98, in train_cifar\n",
            "AttributeError: 'NoneType' object has no attribute 'save_path'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_0289e_00001 errored after 0 iterations at 2024-06-17 02:56:52. Total running time: 1min 42s\n",
            "Error file: /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00001_1_batch_size=8,lr=0.0408_2024-06-17_02-55-11/error.txt\n",
            "\n",
            "Trial train_cifar_0289e_00002 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_0289e_00002 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                     2 |\n",
            "| l1                                             8 |\n",
            "| l2                                             4 |\n",
            "| lr                                       0.01114 |\n",
            "+--------------------------------------------------+\n",
            "\u001b[36m(func pid=6719)\u001b[0m Files already downloaded and verified\n",
            "\u001b[36m(func pid=6719)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=6719)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=6719)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[36m(func pid=6719)\u001b[0m wandb: Currently logged in as: hcc9876 (nayoungpark). Use `wandb login --relogin` to force relogin\n",
            "\u001b[36m(func pid=6719)\u001b[0m wandb: Tracking run with wandb version 0.17.1\n",
            "\u001b[36m(func pid=6719)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/working_dirs/train_cifar_0289e_00002_2_batch_size=2,lr=0.0111_2024-06-17_02-55-11/wandb/run-20240617_025702-uibxgaur\n",
            "\u001b[36m(func pid=6719)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[36m(func pid=6719)\u001b[0m wandb: Syncing run fearless-universe-31\n",
            "\u001b[36m(func pid=6719)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/nayoungpark/torch-turn\n",
            "\u001b[36m(func pid=6719)\u001b[0m wandb: ğŸš€ View run at https://wandb.ai/nayoungpark/torch-turn/runs/uibxgaur\n",
            "\u001b[36m(func pid=6719)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=6719)\u001b[0m   warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 2 ERROR | 1 RUNNING | 7 PENDING\n",
            "Current time: 2024-06-17 02:57:12. Total running time: 2min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00002   RUNNING    0.0111418                2 |\n",
            "| train_cifar_0289e_00003   PENDING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1,  2000] loss: 2.314\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1,  4000] loss: 1.158\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1,  6000] loss: 0.772\n",
            "Trial status: 2 ERROR | 1 RUNNING | 7 PENDING\n",
            "Current time: 2024-06-17 02:57:42. Total running time: 2min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00002   RUNNING    0.0111418                2 |\n",
            "| train_cifar_0289e_00003   PENDING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1,  8000] loss: 0.579\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1, 10000] loss: 0.463\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1, 12000] loss: 0.386\n",
            "Trial status: 2 ERROR | 1 RUNNING | 7 PENDING\n",
            "Current time: 2024-06-17 02:58:12. Total running time: 3min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00002   RUNNING    0.0111418                2 |\n",
            "| train_cifar_0289e_00003   PENDING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1, 14000] loss: 0.331\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1, 16000] loss: 0.289\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1, 18000] loss: 0.257\n",
            "Trial status: 2 ERROR | 1 RUNNING | 7 PENDING\n",
            "Current time: 2024-06-17 02:58:42. Total running time: 3min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00002   RUNNING    0.0111418                2 |\n",
            "| train_cifar_0289e_00003   PENDING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=6719)\u001b[0m [1, 20000] loss: 0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-17 02:59:04,702\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_0289e_00002\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2613, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=6719, ip=172.28.0.12, actor_id=348cfe425f6cd930a679e42601000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 98, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-7-fbecd79468fb>\", line 98, in train_cifar\n",
            "AttributeError: 'NoneType' object has no attribute 'save_path'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_0289e_00002 errored after 0 iterations at 2024-06-17 02:59:04. Total running time: 3min 54s\n",
            "Error file: /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00002_2_batch_size=2,lr=0.0111_2024-06-17_02-55-11/error.txt\n",
            "\n",
            "Trial train_cifar_0289e_00003 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_0289e_00003 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                     4 |\n",
            "| l1                                           128 |\n",
            "| l2                                            32 |\n",
            "| lr                                       0.00022 |\n",
            "+--------------------------------------------------+\n",
            "\u001b[36m(func pid=7456)\u001b[0m Files already downloaded and verified\n",
            "\n",
            "Trial status: 3 ERROR | 1 RUNNING | 6 PENDING\n",
            "Current time: 2024-06-17 02:59:12. Total running time: 4min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00003   RUNNING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=7456)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=7456)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=7456)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[36m(func pid=7456)\u001b[0m wandb: Currently logged in as: hcc9876 (nayoungpark). Use `wandb login --relogin` to force relogin\n",
            "\u001b[36m(func pid=7456)\u001b[0m wandb: Tracking run with wandb version 0.17.1\n",
            "\u001b[36m(func pid=7456)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/working_dirs/train_cifar_0289e_00003_3_batch_size=4,lr=0.0002_2024-06-17_02-55-11/wandb/run-20240617_025914-9gdc6qwn\n",
            "\u001b[36m(func pid=7456)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[36m(func pid=7456)\u001b[0m wandb: Syncing run wild-valley-32\n",
            "\u001b[36m(func pid=7456)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/nayoungpark/torch-turn\n",
            "\u001b[36m(func pid=7456)\u001b[0m wandb: ğŸš€ View run at https://wandb.ai/nayoungpark/torch-turn/runs/9gdc6qwn\n",
            "\u001b[36m(func pid=7456)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=7456)\u001b[0m   warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(func pid=7456)\u001b[0m [1,  2000] loss: 2.303\n",
            "\u001b[36m(func pid=7456)\u001b[0m [1,  4000] loss: 1.148\n",
            "Trial status: 3 ERROR | 1 RUNNING | 6 PENDING\n",
            "Current time: 2024-06-17 02:59:42. Total running time: 4min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00003   RUNNING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=7456)\u001b[0m [1,  6000] loss: 0.750\n",
            "\u001b[36m(func pid=7456)\u001b[0m [1,  8000] loss: 0.518\n",
            "Trial status: 3 ERROR | 1 RUNNING | 6 PENDING\n",
            "Current time: 2024-06-17 03:00:12. Total running time: 5min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00003   RUNNING    0.000215367              4 |\n",
            "| train_cifar_0289e_00004   PENDING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=7456)\u001b[0m [1, 10000] loss: 0.391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-17 03:00:29,017\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_0289e_00003\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2613, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=7456, ip=172.28.0.12, actor_id=a5b8ae8542adccc88f0949c901000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 98, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-7-fbecd79468fb>\", line 98, in train_cifar\n",
            "AttributeError: 'NoneType' object has no attribute 'save_path'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_0289e_00003 errored after 0 iterations at 2024-06-17 03:00:29. Total running time: 5min 18s\n",
            "Error file: /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00003_3_batch_size=4,lr=0.0002_2024-06-17_02-55-11/error.txt\n",
            "\n",
            "Trial train_cifar_0289e_00004 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_0289e_00004 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                     2 |\n",
            "| l1                                            32 |\n",
            "| l2                                             8 |\n",
            "| lr                                       0.00045 |\n",
            "+--------------------------------------------------+\n",
            "\u001b[36m(func pid=7993)\u001b[0m Files already downloaded and verified\n",
            "\u001b[36m(func pid=7993)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=7993)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=7993)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[36m(func pid=7993)\u001b[0m wandb: Currently logged in as: hcc9876 (nayoungpark). Use `wandb login --relogin` to force relogin\n",
            "\u001b[36m(func pid=7993)\u001b[0m wandb: Tracking run with wandb version 0.17.1\n",
            "\u001b[36m(func pid=7993)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/working_dirs/train_cifar_0289e_00004_4_batch_size=2,lr=0.0005_2024-06-17_02-55-11/wandb/run-20240617_030036-5j50jzlj\n",
            "\u001b[36m(func pid=7993)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[36m(func pid=7993)\u001b[0m wandb: Syncing run super-surf-33\n",
            "\u001b[36m(func pid=7993)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/nayoungpark/torch-turn\n",
            "\u001b[36m(func pid=7993)\u001b[0m wandb: ğŸš€ View run at https://wandb.ai/nayoungpark/torch-turn/runs/5j50jzlj\n",
            "\u001b[36m(func pid=7993)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=7993)\u001b[0m   warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 4 ERROR | 1 RUNNING | 5 PENDING\n",
            "Current time: 2024-06-17 03:00:42. Total running time: 5min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00004   RUNNING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "| train_cifar_0289e_00003   ERROR      0.000215367              4 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1,  2000] loss: 2.307\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1,  4000] loss: 1.148\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1,  6000] loss: 0.752\n",
            "Trial status: 4 ERROR | 1 RUNNING | 5 PENDING\n",
            "Current time: 2024-06-17 03:01:12. Total running time: 6min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00004   RUNNING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "| train_cifar_0289e_00003   ERROR      0.000215367              4 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1,  8000] loss: 0.539\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1, 10000] loss: 0.406\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1, 12000] loss: 0.313\n",
            "Trial status: 4 ERROR | 1 RUNNING | 5 PENDING\n",
            "Current time: 2024-06-17 03:01:42. Total running time: 6min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00004   RUNNING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "| train_cifar_0289e_00003   ERROR      0.000215367              4 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1, 14000] loss: 0.257\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1, 16000] loss: 0.218\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1, 18000] loss: 0.189\n",
            "Trial status: 4 ERROR | 1 RUNNING | 5 PENDING\n",
            "Current time: 2024-06-17 03:02:12. Total running time: 7min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00004   RUNNING    0.000454672              2 |\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "| train_cifar_0289e_00003   ERROR      0.000215367              4 |\n",
            "+-----------------------------------------------------------------+\n",
            "\u001b[36m(func pid=7993)\u001b[0m [1, 20000] loss: 0.167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-17 03:02:38,240\tERROR tune_controller.py:1331 -- Trial task failed for trial train_cifar_0289e_00004\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2613, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 861, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=7993, ip=172.28.0.12, actor_id=9f63fb699df7fe37b527e1fc01000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 98, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-7-fbecd79468fb>\", line 98, in train_cifar\n",
            "AttributeError: 'NoneType' object has no attribute 'save_path'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_0289e_00004 errored after 0 iterations at 2024-06-17 03:02:38. Total running time: 7min 27s\n",
            "Error file: /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00004_4_batch_size=2,lr=0.0005_2024-06-17_02-55-11/error.txt\n",
            "\n",
            "Trial status: 5 ERROR | 5 PENDING\n",
            "Current time: 2024-06-17 03:02:42. Total running time: 7min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00005   PENDING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "| train_cifar_0289e_00003   ERROR      0.000215367              4 |\n",
            "| train_cifar_0289e_00004   ERROR      0.000454672              2 |\n",
            "+-----------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_0289e_00005 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_0289e_00005 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                    16 |\n",
            "| l1                                            64 |\n",
            "| l2                                             4 |\n",
            "| lr                                       0.01266 |\n",
            "+--------------------------------------------------+\n",
            "\u001b[36m(func pid=8723)\u001b[0m Files already downloaded and verified\n",
            "\u001b[36m(func pid=8723)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=8723)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=8723)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[36m(func pid=8723)\u001b[0m wandb: Currently logged in as: hcc9876 (nayoungpark). Use `wandb login --relogin` to force relogin\n",
            "\u001b[36m(func pid=8723)\u001b[0m wandb: Tracking run with wandb version 0.17.1\n",
            "\u001b[36m(func pid=8723)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/working_dirs/train_cifar_0289e_00005_5_batch_size=16,lr=0.0127_2024-06-17_02-55-11/wandb/run-20240617_030246-7i3peumu\n",
            "\u001b[36m(func pid=8723)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
            "\u001b[36m(func pid=8723)\u001b[0m wandb: Syncing run spring-tree-34\n",
            "\u001b[36m(func pid=8723)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/nayoungpark/torch-turn\n",
            "\u001b[36m(func pid=8723)\u001b[0m wandb: ğŸš€ View run at https://wandb.ai/nayoungpark/torch-turn/runs/7i3peumu\n",
            "\u001b[36m(func pid=8723)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=8723)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "2024-06-17 03:03:06,065\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2024-06-17 03:03:06,076\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_cifar_2024-06-17_02-55-09' in 0.0083s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 5 ERROR | 1 RUNNING | 4 PENDING\n",
            "Current time: 2024-06-17 03:03:06. Total running time: 7min 55s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------+\n",
            "| Trial name                status              lr     batch_size |\n",
            "+-----------------------------------------------------------------+\n",
            "| train_cifar_0289e_00005   RUNNING    0.0126617               16 |\n",
            "| train_cifar_0289e_00006   PENDING    0.00133331               8 |\n",
            "| train_cifar_0289e_00007   PENDING    0.000125546             16 |\n",
            "| train_cifar_0289e_00008   PENDING    0.000782675              4 |\n",
            "| train_cifar_0289e_00009   PENDING    0.0305181                4 |\n",
            "| train_cifar_0289e_00000   ERROR      0.0468533               16 |\n",
            "| train_cifar_0289e_00001   ERROR      0.0407774                8 |\n",
            "| train_cifar_0289e_00002   ERROR      0.0111418                2 |\n",
            "| train_cifar_0289e_00003   ERROR      0.000215367              4 |\n",
            "| train_cifar_0289e_00004   ERROR      0.000454672              2 |\n",
            "+-----------------------------------------------------------------+\n",
            "\n",
            "Number of errored trials: 5\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                  # failures   error file                                                                                                                                                                                                      |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_0289e_00000              1   /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00000_0_batch_size=16,lr=0.0469_2024-06-17_02-55-10/error.txt |\n",
            "| train_cifar_0289e_00001              1   /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00001_1_batch_size=8,lr=0.0408_2024-06-17_02-55-11/error.txt  |\n",
            "| train_cifar_0289e_00002              1   /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00002_2_batch_size=2,lr=0.0111_2024-06-17_02-55-11/error.txt  |\n",
            "| train_cifar_0289e_00003              1   /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00003_3_batch_size=4,lr=0.0002_2024-06-17_02-55-11/error.txt  |\n",
            "| train_cifar_0289e_00004              1   /tmp/ray/session_2024-06-17_02-55-02_238598_5248/artifacts/2024-06-17_02-55-09/train_cifar_2024-06-17_02-55-09/driver_artifacts/train_cifar_0289e_00004_4_batch_size=2,lr=0.0005_2024-06-17_02-55-11/error.txt  |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f5144bbd75b6>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# WandB ë¡œê·¸ì¸ ë° ë©”ì¸ í•¨ìˆ˜ í˜¸ì¶œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c4deceb4d95d8b5448342ffd1a1441bd51ec103a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-f5144bbd75b6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Ray Tuneì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì‹¤í–‰\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     result = tune.run(\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mresources_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mall_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0mincomplete_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/execution/tune_controller.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1973\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;34m\"\"\"Cleanup trials and callbacks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cleanup_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_experiment_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/execution/tune_controller.py\u001b[0m in \u001b[0;36m_cleanup_trials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    801\u001b[0m                     \u001b[0;34m\"Waiting for actor manager to clean up final state [dedup]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m                 )\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Force cleanup of remaining actors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/actor_manager.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mstart_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_futures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\u001b[0m in \u001b[0;36mauto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mauto_init_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2842\u001b[0m         \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2843\u001b[0m         \u001b[0mtimeout_milliseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2844\u001b[0;31m         ready_ids, remaining_ids = worker.core_worker.wait(\n\u001b[0m\u001b[1;32m   2845\u001b[0m             \u001b[0mray_waitables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}