# **Multi-GPU í•™ìŠµ**
ì˜¤ëŠ˜ë‚ ì˜ ë”¥ëŸ¬ë‹ì€ ì ì  ë” ëŒ€ìš©ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê³  ìˆê¸° ë•Œë¬¸ì—, GPU í™•ë³´ê°€ ì¤‘ìš”í•´ì§€ê³  ìˆë‹¤.

### **ê°œë… ì •ë¦¬**
- Single GPU vs Multi GPU
- GPU vs Node
  - NodeëŠ” í•œ ëŒ€ì˜ ì»´í“¨í„°(ë˜ëŠ” ì‹œìŠ¤í…œ)ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.
- Single Node Single GPU
  - í•œ ëŒ€ì˜ ì»´í“¨í„° í•œ ê°œì˜ GPU
- Single Node Multi GPU
  - í•œ ëŒ€ì˜ ì»´í“¨í„° ì—¬ëŸ¬ê°œì˜ GPU
  - ì£¼ë¡œ ì´ ìƒí™©ì„ ë§ì´ ê²ªìŒ
- Multi Node Multi GPU
  - ì—¬ëŸ¬ëŒ€ì˜ ì»´í“¨í„° ì—¬ëŸ¬ê°œì˜ GPU
  - ê¶ê·¹ì ì¸ ë°©í–¥

## **Multi-GPU í•™ìŠµì˜ ë°©ë²•**
ëª¨ë¸ ë‚˜ëˆ„ê¸°(ë³‘ë ¬í™”), ë°ì´í„° ë‚˜ëˆ„ê¸°
![image](https://github.com/Raymondgwangryeol/Raymondgwangryeol/assets/32587541/87c1a333-ebc2-484d-a6e6-f3422bc06939)
<br>

### **Model parallel**
**â“í€´ì¦ˆ**    
ë‹¤ì¤‘ GPUì— í•™ìŠµì„ ë¶„ì‚°í•˜ëŠ” 2ê°€ì§€ ë°©ë²• ì¤‘, ëª¨ë¸ì„ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ë¬´ì—‡ì´ë¼ ë¶ˆë¦¬ëŠ”ê°€?    
**ğŸ–Š ì •ë‹µ:** Model parallel    
- ìƒê°ë³´ë‹¤ ì˜ˆì „ë¶€í„° ì“´ ë°©ë²•.(AlexNet)
- ëª¨ë¸ì˜ ë³‘ëª©, íŒŒì´í”„ë¼ì¸ì˜ ì–´ë ¤ì›€ ë“±ìœ¼ë¡œ ì¸í•´ ëª¨ë¸ ë³‘ë ¬í™”ëŠ” ê³ ë‚œì´ë„ ê³¼ì œ

### **Model parallelì˜ ì–´ë ¤ì›€**
![image](https://github.com/Raymondgwangryeol/Raymondgwangryeol/assets/32587541/2b7cadb7-6310-4299-9e75-e30e56dd8f4e)
<br>

```python
class ModelParallelResNet50(ResNet):
    def __init__(self, *args, **kwargs):
        super(ModelParallelResNet50, self).__init__(Bottleneck, [3,4,6,3], num_classes=num_classes, *args, **kwargs)

        self.seq1 = nn.Sequential(#ì²« ë²ˆì§¸ ëª¨ë¸ì„ cuda 0ì— í• ë‹¹
                    self.conv1, self.bn1, self.relu, self.maxpool, self.layer1, self.layer2).to('cuda:0')
    
        self.seq2 = nn.Sequential(
                    self.layer3, self.layer4, self.avgpool).to('cuda:1') # ë‘ ë²ˆì§¸ ëª¨ë¸ì„ cuda 1ì— í• ë‹¹
    
        self.fc.to('cuda:0')

   def forward(self, x):  # ë‘ ëª¨ë¸ì„ ì—°ê²°í•˜ê¸°
        x = self.seq2(self.seq1(x).to('cuda:1')) # .to() -> copyí•´ì„œ ë³´ë‚´ì¤€ë‹¤.
        return self.fc(x.view(x.size(0), -1))
# ì´ë ‡ê²Œë§Œ êµ¬í˜„í•˜ë©´ ë³‘ë ¬í™”ê°€ ì˜ ì•ˆ ëœë‹¤. (ë³‘ëª©í˜„ìƒ ë°œìƒ ê°€ëŠ¥)
```

### **Data parallel**
- ë°ì´í„°ë¥¼ ë‚˜ëˆ  GPUì— í• ë‹¹ í›„ ê²°ê³¼ì˜ í‰ê· ì„ ì·¨í•˜ëŠ” ë°©ë²•.
- Minibatchì™€ ìœ ì‚¬, í•œ ë²ˆì— ì—¬ëŸ¬ GPUì—ì„œ ìˆ˜í–‰
  - Minibatchì˜ ë³‘ë ¬í™”ë¥¼ ì‹œí‚¨ë‹¤ê³  ì´í•´í•˜ë©´ í¸í•˜ë‹¤.
![image](https://github.com/Raymondgwangryeol/Raymondgwangryeol/assets/32587541/b8eb04d7-ace8-4649-a2df-9bb86f06903e)
<br>

PyTorchì—ì„œëŠ” ë‘ ê°€ì§€ ë°©ì‹ì˜ Data parallelì„ ì œê³µ.
- **DataParallel**
  - ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ë¶„ë°°í•œ í›„ í‰ê· ì„ ì·¨í•¨.
  - GPU ì‚¬ìš© ë¶ˆê· í˜• ë¬¸ì œ ë°œìƒ(í•˜ë‚˜ì˜ GPUì— í•  ì¼ì„ ì „ê°€ -> GPU í­ë°œ)
  - í•´ë‹¹ GPUì— ë§ì¶°ì„œ Batch Size ê°ì†Œ, ì¤‘ê°„ì— Coordinateí•˜ëŠ” GPUê°€ ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•˜ê¸° ë•Œë¬¸ì— í•œ GPUê°€ ë³‘ëª©, ë°ì´í„° ë¶„ì‚° ë¶ˆê· í˜• ë°œìƒ, GIL
      - **â“í€´ì¦ˆ** ë°ì´í„° ë¶„ì‚° ë¶ˆê· í˜•
        - í•œ GPUê°€ ë³‘ëª©ì´ ë  ë•Œ ë‹¤ë¥¸ GPUë“¤ë„ ê·¸ ë³‘ëª© ë•Œë¬¸ì— ì˜í–¥ì„ ë°›ê²Œ ë˜ëŠ” í˜„ìƒ
      - GIL(Global Interpreter Lock)
        - ìŠ¤ë ˆë“œë¼ë¦¬ ê³µìœ í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì˜ ìì›ì„ globalí•˜ê²Œ lockí•´ë²„ë¦¬ê³ , ë‹¨ í•˜ë‚˜ì˜ ìŠ¤ë ˆë“œì—ë§Œ ì´ ìì›ì— ì ‘ê·¼í•˜ëŠ” ê²ƒì„ í—ˆìš©.
        - ë©€í‹°ìŠ¤ë ˆë“œë¼ í•˜ë”ë¼ë„ í•œ ë²ˆì— í•˜ë‚˜ì˜ ìŠ¤ë ˆë“œë§Œ ì‹¤í–‰
```python
parallel_model = torch.nn.DataParallel(model)
```
<br>

- **DistributeDataParallel**
  - **â“í€´ì¦ˆ** ê° CPUë§ˆë‹¤ Process ìƒì„±í•˜ì—¬ ê°œë³„ GPUì— í• ë‹¹
    - ëª¨ìœ¼ì§€ ì•Šì•„ë„ ê°ì Coordinate ê°€ëŠ¥. ë”°ë¡œ ì²˜ë¦¬ í›„ ë‚˜ì¤‘ì— í‰ê· ì¹˜ë¥¼ ë°˜ì˜í•˜ëŠ” ë°©ì‹.
```python
train_sampler=torch.utils.data.distributed.DistributedSampler(train_data) # DataLoaderì˜ Sampler ì‚¬ìš©
                                                                          # Sampler: indexë¥¼ ì»¨íŠ¸ë¡¤ultiprocessign í•˜ëŠ” ë°©ë²•.
                                                                          # shuffle=Falseì—¬ì•¼ í•¨. (ì§ì ‘ ì¸ë±ìŠ¤ë¥¼ ì»¨íŠ¸ë¡¤í•˜ê¸° ë•Œë¬¸)
                                                                          # DistributedSamplerë¥¼ ì‚¬ìš©í•´ì•¼ í•™ìŠµë°ì´í„°ê°€ ê°ê°ì˜ í”„ë¡œì„¸ìŠ¤ì— ê· ì¼í•˜ê²Œ ë°°ë¶„ë¨.
shuffle = False
pin_memory = True

trainloader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=shuffle, pin_memory=pin_memory, num_workers=3, sampler=train_sampler)

def main():
    n_gpus = torch.cuda.device_count()
    torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus))

def main_worker(gpu, n_gpus):
    image_size = 224
    batch_size = 512
    num_worker = 8
    epochs = ...

    batch_size = int(batch_size/n_gpus) # gpu ê°¯ìˆ˜ë§Œí¼ ë‚˜ëˆ ì¤˜ì•¼ í•¨
    num_worker = int(num_worker/n_gpus)

    torch.distributed.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:2568', world_size=n_gpus, rank=gpu) # ë©€í‹°í”„ë¡œì„¸ì‹± í†µì‹  ê·œì•½ ì •ì˜
    model=MODEL
    torch.cuda.set_device(gpu)
    model=model.cuda(gpu)
    model=torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu]) # DistributedDataParallel ì •ì˜
```
