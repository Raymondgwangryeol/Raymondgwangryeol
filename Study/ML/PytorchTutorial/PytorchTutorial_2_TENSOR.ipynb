{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDYyg4gR2BEKl/cqlFmmvq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#텐서(TENSOR)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "배열이나 행렬과 매우 유사한 특수한 자료구조.\n",
        "\n",
        "**PyTorch**에서는 텐서를 사용하여 모델의 입출력, 그리고 모델의 매개변수들을 부호화(encode)한다.\n",
        "\n",
        "Tensor는 GPU나 다른 하드웨어 가속기에서 실행할 수 있다는 점을 제외하면, NumPy의 ndarray와 유사함.\n",
        "\n",
        "실제로, Tensor랑 ndarray가 종종(?) 동**일한 내부(underly)메모리를 공유할 수 있어서**, data를 복사할 필요가 없다. tensor의 값을 바꾸면, 그 tensor를 numpy로 바꾼 ndarray의 값도 **자동으로 업데이트** 됨.\n",
        "\n",
        "또, tensor는 **자동미분(automatic differentiation)에 최적화** 되어 있음."
      ],
      "metadata": {
        "id": "YPCDhqbFg2qp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY-wTRNwfm5p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **- 텐서 초기화**\n",
        "\n",
        "여러가지 방법으로 tensor를 초기화 할 수 있음."
      ],
      "metadata": {
        "id": "JWf5dpMRjpp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 데이터로부터 직접(directly) 생성**"
      ],
      "metadata": {
        "id": "dTsazzX9jxQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 일단 집어넣으면 자료형은 알아서 유추함\n",
        "data = [[1,2], [3,4]] #아 그럼 tensor는 list네..?\n",
        "                      #그렇다 tensor는 3차원 리스트이다\n",
        "x_data = torch.tensor(data)"
      ],
      "metadata": {
        "id": "xG1HSCWCjwe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. NumPy 배열로부터 생성**\n",
        "\n",
        "tensor ↔ numpy 변환 가능."
      ],
      "metadata": {
        "id": "SZ1DW0NKny3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "S2xe7V-ToDQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 다른 tensor로부터 생성.**\n",
        "\n",
        "메소드 이름 뒤에 '_like'가 붙는 거 사용.(ones_like()라던가)\n",
        "\n",
        "명시적으로 재정의(override)하지 않는다면, 인자로 주어진 텐서의 속성(shape, datatype)을 유지"
      ],
      "metadata": {
        "id": "0FFzu4_tosET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x_data shape: [2, 2], dtype: LongTensor(64비트 int)\n",
        "x_ones = torch.ones_like(x_data)\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\") #torch.ones => 1값을 갖는 tensor 생성.\n",
        "                                      #torch.ones(5) = tensor([1.,1.,1.,1.,1.,])\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float)# x_data 속성 덮어쓰기\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zuECmtsplcO",
        "outputId": "e955d9d6-084f-4dec-e6c3-9ba8a95020b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.0641, 0.8724],\n",
            "        [0.8609, 0.6859]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. 무작위(random) 또는 상수(constant) 값을 사용.**\n",
        "\n",
        "\n",
        "아래 코드의 shape => 텐서의 dimension을 나타내는 튜플."
      ],
      "metadata": {
        "id": "WxLJ4GQUsOVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape=(2,3,) #shape값을 튜플로 미리 지정한 후, tensor 생성시 사용\n",
        "\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhp9ntrMspdz",
        "outputId": "63c9aae2-48ac-4dae-c06f-53ebbf8de487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.6297, 0.2243, 0.3904],\n",
            "        [0.0432, 0.1353, 0.1022]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **- 텐서의 속성(Attribute)**\n",
        "\n",
        "텐서 속성 종류: **모양(shape), 자료형(datatype)**\n",
        "\n",
        "**모양**을 알고 싶을 때는 **.shape**\n",
        "**자료형**을 알고 싶을 때는 **.dtype**"
      ],
      "metadata": {
        "id": "U5JPPbOItjQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\\n\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\\n\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlyJeIf0t8gh",
        "outputId": "697ccc55-03bb-4d1e-9000-3a5775a47ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "\n",
            "Datatype of tensor: torch.float32\n",
            "\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **- 텐서 연산(Operation)**\n",
        "\n",
        "전치(transposing), 인덱싱(indexing) 등 100가지 이상의 텐서 연산을 할 수 있음.\n",
        "\n",
        "각 연산들은 (일반적으로 CPU보다 빠른)**GPU에서 실행할 수 있음**. 코랩에서는 Edit>Notebook>Settings가면 GPU 할당할 수 있음.\n",
        "\n",
        "기본적으로 tensor는 CPU에 생성되는데, .to메소드 사용하면 (GPU의 가용성(availabilty) 확인한 후) GPU로 텐서를 명시적으로 이동시킬 수 있음.\n",
        "\n",
        "**주의 - 큰 텐서들을 장치들 간에 복사하면 시간, 메모리 측면에서 비용이 많이 듦**"
      ],
      "metadata": {
        "id": "GvoRj0XHuj3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to(\"cuda\")"
      ],
      "metadata": {
        "id": "ZHKhDhNQyZB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **- 여러 텐서 연산**"
      ],
      "metadata": {
        "id": "udUzONkjyodz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy식 표준 인덱싱과 슬라이싱\n",
        "#tensor = torch.tensor(tensor)\n",
        "tensor = torch.ones(4,4)\n",
        "print(f\"First row: {tensor[0]}\\n\") #indexing\n",
        "print(f\"First column: {tensor[:, 0]}\\n\") #tensor의 모든 행의 [0]번째 원소\n",
        "print(f\"Last column: {tensor[..., -1]}\") #Ellipsis(...) in NumPy\n",
        "                                         # => 요소나 범위를 지정할 때 Ellipsis( )를 사용하여 중간 차원을 생략할 수 있음\n",
        "                                         #tensor[n]의 중간 index들을 싸그리 흘려보낸 것\n",
        "tensor[:,1] = 0\n",
        "\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cLAjIsWyrZb",
        "outputId": "50940509-00e7-466e-9225-1bdaeccc5bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tensor 합치기\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1) #dim=0, 아래에 붙여준다. dim=1, 같은 행끼리 붙여준다.\n",
        "print(t1)\n",
        "\n",
        "'''\n",
        "torch.cat()은 주어진 차원을 기준으로 주어진 텐서들을 붙입(concatenate)니다.\n",
        "torch.stack()은 새로운 차원으로 주어진 텐서들을 붙입니다.\n",
        "따라서, (3, 4)의 크기(shape)를 갖는 2개의 텐서 A와 B를 붙이는 경우,\n",
        "torch.cat([A, B], dim=0)의 결과는 (6, 4)의 크기(shape)를 갖고, (대괄호를 추가하지 않음. 그대로 갖다 붙이기)\n",
        "torch.stack([A, B], dim=0)의 결과는 (2, 3, 4)의 크기를 갖습니다.(각 A,B에 대괄호 두르고 붙이기)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "rG3zix6a5S4r",
        "outputId": "88b04d52-6a90-46ed-beaa-02deb6d42ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntorch.cat()은 주어진 차원을 기준으로 주어진 텐서들을 붙입(concatenate)니다.\\ntorch.stack()은 새로운 차원으로 주어진 텐서들을 붙입니다.\\n따라서, (3, 4)의 크기(shape)를 갖는 2개의 텐서 A와 B를 붙이는 경우,\\ntorch.cat([A, B], dim=0)의 결과는 (6, 4)의 크기(shape)를 갖고, (대괄호를 추가하지 않음. 그대로 갖다 붙이기)\\ntorch.stack([A, B], dim=0)의 결과는 (2, 3, 4)의 크기를 갖습니다.(각 A,B에 대괄호 두르고 붙이기)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#산술 연산(Arithmetic operations)\n",
        "\n",
        "# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산합니다. y1, y2, y3은 모두 같은 값을 갖습니다.\n",
        "# ``tensor.T`` 는 텐서의 전치(transpose)를 반환합니다.\n",
        "\n",
        "y1 = tensor @ tensor.T #아래와 같은 뜻. @는 행렬곱을 뜻함.\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1) # 같은 shpae끼리 곱해서 그냥 rand_like로 만들고 거기다 값 넣은 듯\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "print(y3)\n",
        "\n",
        "\n",
        "# 요소별 곱(element-wise product)을 계산합니다. z1, z2, z3는 모두 같은 값을 갖습니다.\n",
        "\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)\n",
        "\n",
        "print(z3)\n",
        "\n",
        "#참고 - 내적은 .dot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpvBLzrZ-Bs9",
        "outputId": "971a5c23-43aa-4851-ab12-a4a82ddb7a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단일-요소(single-element) 텐서\n",
        "\n",
        "#텐서의 모든 값을 하나로 집계(aggregate)해서 요소가 하나인 텐서의 경우, item()으로 숫자값 반환 가능.\n",
        "\n",
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4arpYZUDBsdo",
        "outputId": "24408fae-5e78-495a-a949-2dee2fdc32de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0 <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#바꿔치기(in-place)연산\n",
        "\n",
        "# +=, -=랑 똑같슴니다\n",
        "#_접미사를 갖는다\n",
        "\n",
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5) #tensor = tensor * 5\n",
        "print(tensor)\n",
        "\n",
        "#주의 - 바꿔치기 연산은 메모리를 일부 절약하지만,\n",
        "#       기록(history)이 즉시 삭제되어 도함수(derivative) 계산에 문제가 발생할 수 있습니다.\n",
        "#       따라서, 사용을 권장하지 않습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTA6oXK-Cc3o",
        "outputId": "1a8f0d65-c371-4861-8804-f3b902ec6f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    }
  ]
}