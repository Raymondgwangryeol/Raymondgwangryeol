{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **8-2. Weight initialization**\n",
        "------\n",
        "학습이 잘 안 되는 이유들 중 하나만 들어볼까요?  \n",
        "Geoffrey Hinton씨에게 여쭤봤습니다.  \n",
        "- 우리가 weight를 멍청한 방식으로 초기화 한다     \n",
        "\n",
        "Weight 초기화를 잘 해야 좋은 결과가 나오는데, 지금까지는 0으로 초기화 하거나, 임의의 값을 넣는 방식으로 초기화를 해오고 있었다!   \n",
        "0으로 초기화 하는 방식은 굉장히 안 좋은 방식인데, 역전파 단계에서 W를 곱할 때 W가 0이면 gradient가 0이 되기 때문!   \n",
        "\n",
        "그럼 RBM으로 weight 초기화 하는 건 어떤데\n",
        "\n",
        "## **RBM(Restricted Boltsmann Machine)**\n",
        "- Restricted: 레이어 안에서는 연결이 없다\n",
        "- 다른 레이어들 끼리는 Fully-connected\n",
        "- 각 층에서 계산된 Weight를 인코딩(forward)/ 디코딩(backward)\n",
        "\n",
        "### **Deep Belief Network - RBM을 가중치 초기화에 적용**\n",
        "Pre - training 단계에서 RBM 적용   \n",
        "stack 쌓는다고 생각하면 편함!   \n",
        "\n",
        "1. 일단 무작위로 Weight를 초기화 시킨다.\n",
        "2. 2개의 layer를 RBM으로 학습한다.\n",
        "3. 2개의 layer위에 한 층을 더 쌓고, 2번째 layer와 새로 쌓은 3번째 layer끼리 RBM으로 학습을 진행한다. 1번째 층과 2번째 층에서의 학습으로 나온 W값은 고정된다.\n",
        "4. 3번을 반복한다.\n",
        "5. 구해진 W를 가지고 전체 층에 역전파 시킨다.\n",
        "\n",
        "그래서 RBM 지금도 많이 쓰나요?    \n",
        "아니요. RBM 너무 복잡해요...    \n",
        "RBM보다 더 간단하게 Weight를 초기화 할 수 있는 방법 없을까?   \n",
        "\n",
        "## **Xavier(2010) / He(2015) initialization**\n",
        "수식을 이용한 초기화\n",
        "### **Xavier**\n",
        "Layer의 특징에 따라서 초기화를 해야 한다!\n",
        "n_in: layer input 수\n",
        "n_out: layer ouput 수\n",
        "- Nomal initalization   \n",
        "$$W∼N(0, Var(W))$$\n",
        "$$Var(W) = \\sqrt{\\frac{2}{n_{in} + n_{out}}}$$\n",
        "- Uniform initialization\n",
        "$$W∼U(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, +\\sqrt{\\frac{6}{n_{in} + n_{out}}})$$\n",
        "\n",
        "### **He**\n",
        "Xavier의 변형, n_out 안 쓰는 Xavier라 생각하면 됨.\n",
        "- Nomal initalization   \n",
        "$$W∼N(0, Var(W))$$\n",
        "$$Var(W) = \\sqrt{\\frac{2}{n_{in}}}$$\n",
        "- Uniform initialization\n",
        "$$W∼U(-\\sqrt{\\frac{6}{n_{in}}}, +\\sqrt{\\frac{6}{n_{in}}})$$"
      ],
      "metadata": {
        "id": "Nd_-k6VpSCv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code: mnist_nn_xavier**"
      ],
      "metadata": {
        "id": "jQ_0kS6_XCJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko_17mIcynvy"
      },
      "outputs": [],
      "source": [
        "def xavier_uniform_(tensor, gain=1): #gain = Var(W)\n",
        "    fan_in, fan_out = calculate_fan_in_and_fan_out # n_in, n_out\n",
        "    std = gain * math.sqrt(2.0 / (fan_in + fan_out))\n",
        "    a= math.sqrt(3.0) * std #uniform initialization 형태\n",
        "    with torch.no_grad():\n",
        "        return tensor.uniform_(-a, a) #uniform distribution 초기화"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ],
      "metadata": {
        "id": "av9ylFfQkWJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#for reproducibility\n",
        "random.seed(777) # 데이터 전처리 및 분할\n",
        "torch.manual_seed(777) #랜덤 초기화\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all"
      ],
      "metadata": {
        "id": "5S5yXcoqk9iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "TpfYO8j_qWOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mnist dataset\n",
        "mnist_train = dsets.MNIST(root='./',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "mnist_test = dsets.MNIST(root='./',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "metadata": {
        "id": "V-8_V_09qhdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset = mnist_train,\n",
        "                                    batch_size = batch_size,\n",
        "                                    shuffle = True,\n",
        "                                    drop_last = True)"
      ],
      "metadata": {
        "id": "hAyRBmTgrLdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nn layers\n",
        "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
        "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
        "linear5 = torch.nn.Linear(512, 10 , bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "#Xaviar initialization\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "torch.nn.init.xavier_uniform_(linear4.weight)\n",
        "torch.nn.init.xavier_uniform_(linear5.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq5UhfGgrpOe",
        "outputId": "4bae3b6f-ce07-4a9a-93bd-ab4683940849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0565,  0.0423, -0.0155,  ...,  0.1012,  0.0459, -0.0191],\n",
              "        [ 0.0772,  0.0452, -0.0638,  ...,  0.0476, -0.0638,  0.0528],\n",
              "        [ 0.0311, -0.1023, -0.0701,  ...,  0.0412, -0.1004,  0.0738],\n",
              "        ...,\n",
              "        [ 0.0334,  0.0187, -0.1021,  ...,  0.0280, -0.0583, -0.1018],\n",
              "        [-0.0506, -0.0939, -0.0467,  ..., -0.0554, -0.0325,  0.0640],\n",
              "        [-0.0183, -0.0123,  0.1025,  ..., -0.0214,  0.0220, -0.0741]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3, relu, linear4, relu, linear5).to(device)"
      ],
      "metadata": {
        "id": "1i8Df2q_sd9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer, cost\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "WQSMoN5cstml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3mR0KH_tDmL",
        "outputId": "16abd1b2-d94a-4d84-a6c4-52fc600217c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 cost = 0.374631554\n",
            "Epoch: 0002 cost = 0.212149188\n",
            "Epoch: 0003 cost = 0.182468370\n",
            "Epoch: 0004 cost = 0.158700451\n",
            "Epoch: 0005 cost = 0.145320401\n",
            "Epoch: 0006 cost = 0.143682033\n",
            "Epoch: 0007 cost = 0.151302010\n",
            "Epoch: 0008 cost = 0.131958067\n",
            "Epoch: 0009 cost = 0.117567852\n",
            "Epoch: 0010 cost = 0.112223327\n",
            "Epoch: 0011 cost = 0.108900353\n",
            "Epoch: 0012 cost = 0.099287510\n",
            "Epoch: 0013 cost = 0.103652336\n",
            "Epoch: 0014 cost = 0.090350568\n",
            "Epoch: 0015 cost = 0.103107490\n",
            "Learning finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the model\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test#dim=1\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    #Get one and predict\n",
        "    r = random.randint(0, len(mnist_test)-1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BOFGBWEtg0t",
        "outputId": "d19027cb-663e-4e0c-e5a0-31d1de363f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9639999866485596\n",
            "Label:  8\n",
            "Prediction:  7\n"
          ]
        }
      ]
    }
  ]
}
