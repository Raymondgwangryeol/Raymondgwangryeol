{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpzcRziiNfmB"
   },
   "source": [
    "# **Softmax Classification**\n",
    "--------\n",
    "Max 값을 Soft하게 붙여줘요! 와!\n",
    "\n",
    "## **SoftMax**\n",
    "분포의 종류로는, 함수의 면적이 확률 값을 나타내는 정규분포도 있고, 특정 범위에서(점) 값이 일정한 균등 분포(Uniform Distribution)도 있다.   \n",
    "균등 분포를 가진 데이터를 다룰 때, SoftMax 활성화 함수를 써서 확률을 구하게 된다!\n",
    "<br>\n",
    "\n",
    "#### **균등 분포의 예)**\n",
    "- **주사위 던지기(이산 균등 분포)**\n",
    "    - 확률 함수가 정의돈 모든 곳에서 그 값이 일정한 분포\n",
    "    - A는 0.7의 확률로 무조건(?) 가위를 낸다\n",
    "- **주사위 던지기(연속 균등 분포)**\n",
    "    - 특정 범위 내에서 확률이 균등하게 분포\n",
    "    - 주사위를 던졌을 때 숫자 6이 나올 확률은 1/6\n",
    "<br><br>\n",
    "\n",
    "예를 들어, 친구랑 매일 천원 내기를 가위바위보 삼세판으로 한다고 치자.   \n",
    "근데 내가 맨날 진다...... 분하다   \n",
    "그래서 이 자식을 관찰해 보니 가위를 제일 잘 냈다.   \n",
    "독이 오른 나는 얘가 가위를 냈을 때, 다음에 어떤 걸 낼지 확률을 계산해서 가장 확률이 높은 것을 내려고 한다.   \n",
    "친구가 가위를 낸 다음에 각각 가위, 바위, 보를 내는 확률의 분포가 있을건데...   \n",
    "모델을 학습시켜 그 확률 분포와 비슷하게 근사하고 싶다!! 이 방법으로 최대값을 소프트하게 뽑아 내는 거야!!  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;👉 **SoftMax**\n",
    "<br>\n",
    "<br>\n",
    "즉, 어떤 균등한 확률 분포가 존재할 때, 각 점에서의 확률 값을 적절한 비율로 나타낸다.  \n",
    "이 비율들을 다 더하면 1이 됨!!(Sigmoid와 다른 점!!)   \n",
    "SoftMax를 모델의 맨 마지막 층으로 깔면, 각각의 확률 값을 얻을 수 있기 때문에 다중 분류 문제에 사용된다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibR7YQLNNT3z",
    "outputId": "b2944115-567f-4f72-e726-bfdd49ee63fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "z = torch.FloatTensor([1, 2, 3]) #만약 단순하게 가장 큰 값만 뽑았다면, 해당 확률은 (0, 0, 1)이 될 것임. 하지만 소프트 맥스는 그러지 않지\n",
    "hypothesis = F.softmax(z, dim = 0) #열 방향으로 확률의 합이 1이 됨\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f95Cqiy7lfER",
    "outputId": "90c0c3da-797b-44dc-89a7-3c014682be74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2R1c098lnW8"
   },
   "source": [
    "## **Cross Entropy**\n",
    "2개의 확률 분포가 주어졌을 때, 그 확률 분포들이 얼마나 비슷한지 나타낼 수 있는 수치.   \n",
    "근사하고 싶은 확률 분포를 향해 차이를 줄여나간다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b_bhYfvqQrY"
   },
   "source": [
    "### **Cross Entropy Loss(Low-level)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMgX5zHQlfgM",
    "outputId": "b92fa32a-fc52-41c8-9391-0573e130edbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2025, 0.2176, 0.2324, 0.1142, 0.2333],\n",
      "        [0.1503, 0.1465, 0.2203, 0.1962, 0.2866],\n",
      "        [0.1473, 0.1262, 0.2831, 0.1827, 0.2607]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "hypothesis = F.softmax(z, dim=1) #행을 기준으로 더해서 1\n",
    "print(hypothesis) #예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFpfF5JlpSJc",
    "outputId": "36180eb9-9985-4c6b-b05e-cc4b7c4956a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "#아무 수나 뽑아서 정답 만들기\n",
    "y= torch.randint(5, (3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtF1rb74qapo",
    "outputId": "08f575ef-3cd1-4539-ca2e-d3ddf5175d3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정답을 one-hot vector 형태로 나타내고, 정답 인덱스에 1을, 나머지에 0을 뿌린다.\n",
    "y_one_hot = torch.zeros_like(hypothesis)#(3, 5)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) #(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qia1hdaBsOhS",
    "outputId": "3d475223-302d-4c19-990c-2d76d5121b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4572, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * - torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icpElYLu1kfJ"
   },
   "source": [
    "### **Cross-Entropy Loss with torch.nn.functional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a6mGnUJ15y8",
    "outputId": "785bf99e-8c0c-4806-e9e4-5e345fa55e17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5971, -1.5252, -1.4591, -2.1701, -1.4553],\n",
       "        [-1.8948, -1.9205, -1.5129, -1.6285, -1.2496],\n",
       "        [-1.9153, -2.0702, -1.2618, -1.6998, -1.3445]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -torch.log(hypothesis)와 같은 결과\n",
    "torch.log(F.softmax(z, dim=1))\n",
    "#아래와 같이 하나로 줄일 수 있다\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1gyC6JN2NHz",
    "outputId": "d1fcbf27-5860-41ed-b9e9-a82a0c85555a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4572, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(y_one_hot * -torch.log(F.softmax(z, dim=1)))와 같다\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGM16dIy3jZA",
    "outputId": "29ba6207-7439-4c04-b29a-b145bfecf2a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4572, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F.softmax와 F.nll_loss를 합친 함수는 F.cross_entropy()임\n",
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjUbWkul4EGn"
   },
   "source": [
    "### **Training with Low-level Cross Entropy loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oCc6kfQ94fS8"
   },
   "outputs": [],
   "source": [
    "#label은 0, 1, 2\n",
    "x_train = [[1, 2, 1, 1], # 여기서는 (8, 4)지만, (m, 4)개가 있다고 쳐보자 (m은 큰 수)\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0] #(m,)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5Y366xu4zVG",
    "outputId": "6df1111c-3f77-4080-f53b-df1e27bdf900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/1000 Cost: 0.902463\n",
      "Epoch  200/1000 Cost: 0.839531\n",
      "Epoch  300/1000 Cost: 0.808064\n",
      "Epoch  400/1000 Cost: 0.788631\n",
      "Epoch  500/1000 Cost: 0.774939\n",
      "Epoch  600/1000 Cost: 0.764541\n",
      "Epoch  700/1000 Cost: 0.756266\n",
      "Epoch  800/1000 Cost: 0.749461\n",
      "Epoch  900/1000 Cost: 0.743724\n",
      "Epoch 1000/1000 Cost: 0.738794\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    hypothesis = F.softmax(x_train.matmul(W)+b, dim = 1)\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "    cost = (y_one_hot * - torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8FrWKV65ORp"
   },
   "source": [
    "### **Training with F.cross_entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZRz2SJb7ukN",
    "outputId": "995831bc-2284-42c2-8df2-6c823cf7a1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/1000 Cost: 0.714269\n",
      "Epoch  200/1000 Cost: 0.638430\n",
      "Epoch  300/1000 Cost: 0.591185\n",
      "Epoch  400/1000 Cost: 0.554105\n",
      "Epoch  500/1000 Cost: 0.522028\n",
      "Epoch  600/1000 Cost: 0.492792\n",
      "Epoch  700/1000 Cost: 0.465232\n",
      "Epoch  800/1000 Cost: 0.438588\n",
      "Epoch  900/1000 Cost: 0.412274\n",
      "Epoch 1000/1000 Cost: 0.385798\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpxYDInc6qT2"
   },
   "source": [
    "### **High-level Implementation with nn.Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4QIk8cn6zgz",
    "outputId": "83fb9ef3-bd3e-434a-99a9-9162ebf8ac12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/1000 Cost: 0.708336\n",
      "Epoch  200/1000 Cost: 0.626136\n",
      "Epoch  300/1000 Cost: 0.569848\n",
      "Epoch  400/1000 Cost: 0.520366\n",
      "Epoch  500/1000 Cost: 0.473517\n",
      "Epoch  600/1000 Cost: 0.427770\n",
      "Epoch  700/1000 Cost: 0.382397\n",
      "Epoch  800/1000 Cost: 0.337085\n",
      "Epoch  900/1000 Cost: 0.292189\n",
      "Epoch 1000/1000 Cost: 0.251781\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SoftmaxClassifierModel()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
