# 2장. 딥러닝과 신경망   
강의: 세종대학교 최유경 교수님 딥러닝 시스템(2023)    
</br><br>
## 목차
1. 퍼셉트론
2. 다층 퍼셉트론
3. 활성화 
4. 순방향 계산
5. 오차 
6. 최적화
7. 역전파 알고리즘

</br></br><br>

## 2-1. 퍼셉트론
### ■ Perceptron?
  **퍼셉트론**은 뉴런이 하나뿐인 가장 간단한 형태의 신경망을 말함   
  신경망은 많은 수의 뉴런으로 이루어져 있고, 각 뉴런은 층 모양으로 배열됨. 해당 구조를 다층 퍼셉트론(=신경망)이라 함.
  <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-2.png"></img><br>
  </br><br>
  #### 생물학적 뉴런 vs 인공 뉴런    
  <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-3.png"><br>
  - 인공뉴런은 생물학적 뉴럭에서 일어나는 현상을 모형화 한 것   
  - 수상돌기로 부터 다른 세기의 전기적 신호를 받음(입력) → 신호 세기 합이(가중합) 정해진 임계값을 넘으면 시냅스를 통해 출련 신호 내보냄(스텝 함수)     
  - 입력 특징에 따라 중요도가 부여, 중요도 값을 결합 가중치라고 함    
  <br><br>  
  #### 퍼셉트론 구성요소    
  <br><img src="https://velog.velcdn.com/images%2Fwoongstar%2Fpost%2Fc7d25a0f-c301-41e1-af57-411c5a5d44a7%2FUntitled%202.png"><br>    
   - 입력 벡터: 보통 대문자 X로 나타냄    
   - 가중치 벡터: 각각의 입력에 부여되는 가중치값(w). 출력에 대한 입력의 중요도를 나타냄    
   - **뉴런 함수**: 입력 신호를 변환하기 위해 가중합 함수와 활성화 함수 사용    
   - 출력: 활성화 함수에 의해 결정, 이진수, 확률, 또는 실수값 출력 가능     
  <br>
  
  ### ■ 뉴런 함수
   #### 가중합 함수
   선형 결합(Linear combination)이라고도 하며, 각 가중치를 곱한 입력 값의 합에 편향을 더한 값     
   값이 클수록 우선순위가 높음     
   z = Σxi · wi + b (bias)    
  <br>
  
   #### 활성화 함수
   가중합을 입력 받아, 가중합이 미리 정해진 임계값보다 크면 뉴런을 활성화. 결정을 내리는 뇌의 역할.    
   #### 편향이란?
   편향(b)값을 조정해 더 정확한 데이터 예측이 되도록 직선의 위치를 조절할 수 있음   
   신경망 학습 과정에서 편향은 추가된 가중치로 취급되며, 비용함수가 최소가 되도록 최적화 함    
   항상 값이 1인 입력 노드를 추가하는 방법으로 입력 층에도 편향 도입 가능.    
   <br>
   
  ### ■ 시행착오 전략
   퍼셉트론은 실수를 통해 배우는 시행착오 전략으로 학습.    
   손실 함수 값을 최소가 되게 하도록 가중치를 조절함.   
   입력의 가중합 계산, 활성화 함수에 입력 → 예측값 결정(순방향 계산) → 예측 값과 실제 레이블 값을 비교, 오차 계산 → 오차에 따라 가중치 조정, 오차가 0에 가깝도록 과정 반복   
    <br>
    
## 2-2. 다층 퍼셉트론(Multilayer Perceptron, MLP)
 퍼셉트론은 직선 함수기 때문에, 선형 데이터는 분류할 수 있지만 하나의 뉴런만으로 비선형 데이터를 처리하기 어려움.   
 비선형 데이터를 다루기 위해 적어도 2개 이상의 직선 필요.  
 은닉층이 여러 층으로 구성. 층끼리는 서로 가중치 결합을 통해 연결되어 있음.    
  <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-10.png"></img><br>
    
 #### 다층 퍼셉트론의 구조
 - 입력층    
 - 은닉층: 실제로 데이터가 학습되는 곳. 복잡한 데이터를 다룰 수록 은닉층을 늘림. 뒤쪽으로 갈수록 복잡한 특징을 학습    
 - 결합 가중치: 노드 간 연결에 입력에 영향력을 나타내는 가중치가 부여     
 - 출력층: 뉴런에서 사용하는 활성화 함수의 종류에 따라 결정    
 </br></br>
 
 #### 신경망 설계
  신경망의 층수와 각 층을 이루는 노드 수를 결정. 층수가 많을수록 계산 비용이 많이 소요.     
  다른 사람의 구현에 성공한 신경망 설계(baseline)을 참고해 출발점을 잡는게 좋음(튜닝).     
  심층 신경망 = 은닉층이 2개 이상인 신경망    
  신경망의 층수가 많을 수록 복잡한 학습 데이터에 부합. 은닉층을 무작정 늘리면 overfitting 발생 가능, 새로운 데이터에 대한 예측이 오히려 부정확해짐.    
  </br>
  
 #### 전결합층(Fully Connected layer, FC)
  고전적 다층 퍼셉트론에서 특히 중요시 되는 구조. 이전 층의 모든 노드가 각기 다른 층의 모든 노드와 연결 됨    
 <br>
 
 #### 신경망 하이퍼파라미터 관련 중요 사항
 1. 은닉층 수    
  작은 규모의 신경망으로 시작해서 성능에 따라 차츰 층을 늘려가는 게 좋음. 학습 데이터와 테스트 데이터의 정확도가 모두 높아지는 지점까지만 학습하는게 좋음. (과적합 예방) 
 2. 활성화 함수    
  ReLU, Softmax 함수가 가장 널리 사용되며, ReLU는 출력층에, Softmax는 출력층에 사용하는 것이 좋음.
 3. 오차 함수    
  회귀 문제에서는 평균제곱오차(Mean Square Error)가 많이 사용됨.    
  분류 문제에서는 교차 엔트로피(Cross Entropy)가 많이 사용.    
 4. 최적화 기법
  배치 경사 하강법, 확률적 경사 하강법, 미니배치 경사 하강법, **Adam, RMSprop** 등이 많이 사용됨. 한 번씩 써보고 가장 좋은 알고리즘을 찾음.
 5. 배치 크기    
  파라미터를 한번 업데이트 할 때 마다 신경망에 입력되는 학습 데이터의 수를 말하며, 미니배치 경사 하강법에 큰 영향을 줌.   
  배치 크기가 클수록 학습 시간이 빨라지나, 메모리 용량이 많이 필요해짐.
  기본값 32로 시작해서 64, 128, 256으로 늘리는 것을 추천      
 6. 에포크 수   
  테스트 데이터에 대한 정확도가 하락하기 시작할 때 까지 에포크 수를 조금씩 늘려가는 것이 좋음    
 7. 학습률
  최적화 알고리즘의 파라미터로, 이론상 학습률이 충분히 작아야 최적 파라미터에 도달할 수 있으나, 무조건 작으면 도달하기까지가 오래 걸릴 수 있음.    
  일반적으로 딥러닝 라이브러리에 지정된 기본값부터 출발하는 것이 적당하며, 자릿수를 하나씩 올리거나 내리는 식으로 조절하며 변경하는 것이 좋음

 </br></br>
 
 ## 2-3. 활성화 함수
 선형 모델을 아무리 여러개 놓고 학습 해봤자 결과는 선형 모델임. 선형 결합을 비선형 모델로 만들어 주는 과정이 필요한데, 이 역할을 해주는 게 활성화 함수임.   
 출력값을 특정 구간 안으로 제약하는 효과도 있음.    
 전이 함수, 비선형성이라고도 함.    
 </br>
 #### 활성화 함수 종류
 1. 선형 전달 함수(Linear transfer function)
  입력을 그대로 출력하는 함수. 가중 평균의 배율을 조정하는 효과는 있으나, 활성화 함수의 역할을 하지 목함.    
  선형 함수의 도함수는 상수기 때문에 입력과 아무 상관이 없고, 역전파 기울기도 항상 동일한 값이 되서 오차를 줄일 수 없기 때문에, 좋은 학습 모델이 아님.       
 </br>
 
 2. 스텝 함수(Step Function)
  이진 분류 문제에 주로 쓰이며, 출력값은 0,1임.      
  x > 0 이면 1 출력     
  x < 0 이면 0 출력     
  </br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-14.png"></img></br>
  
 3. 시그모이드 함수/로지스틱 함수      
  모든 입력값을 특정 구간(0~1)로 바꿔주는 함수. 때문에 극단적인 출력 값이나 예외 출력값이 존재하지 않음.    
  함수 그래프가 매끄러운 S자 모양 곡선     
  주로 이진 분류에서 두 클래스의 확률을 구할 때 사용.     
  <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-15.png"></img><br>
  
  4. 소프트맥스 함수     
   시그모이드 함수의 일반형. 즉, 3개 이상의 클래스에 대해 확률을 구할 때 사용.     
   출력 값은 0에서 1사이의 값이고, 각 클래스 확률의 총 합은 항상 1임. 주로 다중 분류 문제에서 사용.     
   <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-16.png"></img><br>
   
  5. 하이퍼볼릭 탄젠트 함수
    시그모이드 함수를 y축 이동한 함수로, -1부터 1 사이의 값을 출력.    
    데이터의 평균이 0.5(0~1사이 값)인 시그모이드와 달리 tanh 함수는 평균이 0에 가까워지기 때문에 데이터를 중앙에 모으는 효과가 있어 다음층의 학습에 더 유리함.  
    시그모이드, 탄젠트 함수 모두 입력 값이 매우 크거나 작을 경우, 함수의 기울기가 매우 작아지기 때문에 경사 하강법을 이용한 학습이 느려지게 함. ReLU 활성화 함수를 이용해 문제 해결 가능.     
    <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-17.png"></img><br>
    
  6. 정류 선형 유닛(Rectified Linear Unit, ReLU)
   입력이 0보다 작으면 0을, 0보다 크면 자기 자신을 변환하는 함수. 최고 성능의 활성화 함수로 평가받고 있음. 은닉층에서 tanh나 sigmoid보다 더 높은 성능을 보임. Gradient Vanishing 문제를 해결할 수 있으나, 0보다 작은 값이 들어오면 0으로 처리하기 때문에 죽은 뉴런 현상을 초래할 수 있음.      
    <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-18.png"></img><br>
   
   7. 누설 정류 선형 유닛 함수(Leaky ReLU)
    ReLU 함수의 단점을 보완한 활성화 함수. ReLU의 기울기 포화 문제를 해결하기 위해 제안됨.      
    입력이 음수인 구간에서 함수 값이 0이 되지 않도록 음수에 아주 작은 수(강의에서는 0.01를 써도 되지만 권장하지 않는다고 하심)를 곱함. 드물게 사용 됨.
    </br></br>
    

## 2-4. 순방향 계산(Feedforward Process)
 특징의 선형 결합은 활성화 함수에 통과시키는 계산 과정을 말함. 가중합과 활성화 함수를 연이어 계산함.    
 작은 신경망이어도 계산이 복잡해지기 때문에 행렬을 이용해 큰 규모의 계산을 빠르게 수행 할 수 있음.     
 은닉층의 노드는 각 층에서 학습된 새로운 특징을 의미하는데, 해당 노드들을 볼수도, 제어할 수도 없기 때문에 만족스러운 정확도가 나올 때 까지 파라미터를 조정하여 은닉층을 학습시키는 수밖에 없음.       
 </br></br></br>
 
 
## 2-5. 오차 함수(Error function)
 비용함수(cost function), 손실 함수(Loss function)라고도 부름. 오차 함수를 이용해 예측 결과가 정답간의 오차를 측정, 모델의 정확도 파악 가능함.       
오차를 최소로 만드는 과정을 오차 함수 최적화라고 함.
</br>

예측 오차가 양수여야 평균 계산할 때 오차끼리 서로 상쇄되는 일이 발생하지 않기 때문에, **오차의 값은 언제나 양수**임    
</br>

교차 엔트로피(Cross-entropy)는 두 확률 분포 간의 차이를 측정할 수 있다는 특성 때문에 **분류 문제**에서 많이 사용. 
</br><img src="https://drek4537l1klr.cloudfront.net/elgendy/Figures/02_28_F4b.png"></img></br>

오차를 줄이기 위해 가중치를 조절, 목표 지점을 찾아가는게 모델 학습의 목표임.     
</br></br></br>

## 2-6. 최적화 알고리즘
 최적화란 어떤 값을 최소화 하거나 최대화 하는 과정을 통해 최적의 가정치를 찾는 과정을 말함. 오차 함수의 큰 이점은 신경망 학습을 오차를 최소화 하는 최적화 문제로 재정의 할 수 있다는 것.    </br> 
 
 ### ■ 경사 하강법
  가중치를 반복하여 수정하면서, 오차 함수의 최저점에 도달할 때 까지 오차 함수의 언덕을 내려가는 과정.    
  경사 = 미분(곡선에 대한 접선이 갖는 변화율 또는 기울기)      </br>
무작위로 결정한 가중치의 초기값에서 오차함수 값이 최소가 되는 지점에 도달하기 위해 한 번 돌릴 때 마다 방향(경사)와 보폭(학습률)을 알아내야 함.     
</br>

#### 방향(경사)
 오차 함수의 지점에서 경사가 가장 가파른 방향으로 이동.    
#### 보복(학습률)     
 보통 알파로 나타내며, 학습률이 크며 신경망의 학습이 빠르게 진행됨. 하지만 너무 크면 오차값이 감소하지 않고 진동할 수 있어서 적당히 작은 학습률을 설정하는 것이 중요. 큰 값부터 시작해서 점차 낮춰가는 것이 좋음.    
</br>

가중치 변화량 = 방향(경사) x 보폭(학습률)
</br>
<img src="https://drek4537l1klr.cloudfront.net/elgendy/Figures/02_38_F1.png"></img></br>
기호가 마이너스인 이유는 올라가는 방향이 아닌 경사를 내려가는 방법이기 때문에 경사의 반대 방향으로 이동해야 하기 때문.     
</br>

 1. 배치 경사 하강법 단점
  가중치 초기값에 따라 최소점이 아니라 극소점에 빠질 수 있음    
  경사 계산을 위해 매번 훈련 데이처 전체를 사용하기 때문에 계산 비용이 너무 크고 속도도 느림.(대규모 데이터에 적합하지 않음)    
 </br>
 
 2. 확률정 경사 하강법(Stochastic Gradient Descent, SGD)
  머신러닝에서 가장 널리 쓰이는 최적화 알고리즘. 무작위로 시작점으로 골라서 가중치를 수정. 배치 경사 하강법과 달리, 다양한 시작점을 만들 수 있고 여러지역 극소점을 발견할 수 있음. 발견된 여러 극소점 중 가장 작은 값을 전역 최소점으로 삼음. 오차 함수를 따라 하강하는 경로가 진동하는 패턴을 보임.  
  </br>
  
  #### 확률적 경사 하강법 과정
 훈련 데이터 무작위로 섞기 → 데이터를 하나 선택해서 입력 → 경사 계산 → 가중치 수정 → 또 다른 데이터를 하나 선택해서 입력 → 반복     
  
 3. 미니배치 경사 하강법
  배치 경사 하강법과 확률적 경사 하강법의 절충안. 모든 훈련 데이터(배치 경사 하강법)나 하나의 훈련 데이터(확률적 경사 하강법)을 사용하는게 아니고, 데이터를 동일한 크기의 미니배치로(256이 흔함) 분할 후 미니배치의 경사를 계산. 모든 훈련 데이터가 입력될 때 까지 반복.    
  </br>
  배치 경사 하강법 보다 가중치 수정 횟수는 더 많지만 더 적은 반복 횟수에서 가중치가 수렴. 또한 벡터 연산을 사용할 수 있기 때문에 확률적 경사 하강보다 계산 효율이 좋음.
 </br></br></br>
 
 ## 2-7. 역전파 알고리즘
  경사 하강법을 이용해서 가중치를 수정할 때 역정파 알고리즘이 사용됨. 가중치에 대한 오차의 미분을 신경망 전체에 반대 방향으로 전파해서 가중치를 수정함.
  </br><img src="https://drek4537l1klr.cloudfront.net/elgendy/Figures/2-unnumb-23.png"></img></br>
  오차 함수에 바로 연결되지 않은 가중치에 대한 오차의 변화는 연쇄법칙을 사용해 계산.



 
    
    
    
    
  
  
    
  
