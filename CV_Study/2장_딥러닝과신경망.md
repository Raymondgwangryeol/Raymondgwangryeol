# 2장. 딥러닝과 신경망   
강의: 세종대학교 최유경 교수님 딥러닝 시스템(2023)    
</br><br>
## 목차
1. 퍼셉트론
2. 다층 퍼셉트론
3. 활성화 

</br></br><br>

## 2-1. 퍼셉트론
### ■ Perceptron?
  **퍼셉트론**은 뉴런이 하나뿐인 가장 간단한 형태의 신경망을 말함   
  신경망은 많은 수의 뉴런으로 이루어져 있고, 각 뉴런은 층 모양으로 배열됨. 해당 구조를 다층 퍼셉트론(=신경망)이라 함.
  <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-2.png"></img><br>
  </br><br>
  #### 생물학적 뉴런 vs 인공 뉴런    
  <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-3.png"><br>
  - 인공뉴런은 생물학적 뉴럭에서 일어나는 현상을 모형화 한 것   
  - 수상돌기로 부터 다른 세기의 전기적 신호를 받음(입력) → 신호 세기 합이(가중합) 정해진 임계값을 넘으면 시냅스를 통해 출련 신호 내보냄(스텝 함수)     
  - 입력 특징에 따라 중요도가 부여, 중요도 값을 결합 가중치라고 함    
  <br><br>  
  #### 퍼셉트론 구성요소    
  <br><img src="https://velog.velcdn.com/images%2Fwoongstar%2Fpost%2Fc7d25a0f-c301-41e1-af57-411c5a5d44a7%2FUntitled%202.png"><br>    
   - 입력 벡터: 보통 대문자 X로 나타냄    
   - 가중치 벡터: 각각의 입력에 부여되는 가중치값(w). 출력에 대한 입력의 중요도를 나타냄    
   - **뉴런 함수**: 입력 신호를 변환하기 위해 가중합 함수와 활성화 함수 사용    
   - 출력: 활성화 함수에 의해 결정, 이진수, 확률, 또는 실수값 출력 가능     
  <br>
  
  ### ■ 뉴런 함수
   #### 가중합 함수
   선형 결합(Linear combination)이라고도 하며, 각 가중치를 곱한 입력 값의 합에 편향을 더한 값     
   값이 클수록 우선순위가 높음     
   z = Σxi · wi + b (bias)    
  <br>
  
   #### 활성화 함수
   가중합을 입력 받아, 가중합이 미리 정해진 임계값보다 크면 뉴런을 활성화. 결정을 내리는 뇌의 역할.    
   #### 편향이란?
   편향(b)값을 조정해 더 정확한 데이터 예측이 되도록 직선의 위치를 조절할 수 있음   
   신경망 학습 과정에서 편향은 추가된 가중치로 취급되며, 비용함수가 최소가 되도록 최적화 함    
   항상 값이 1인 입력 노드를 추가하는 방법으로 입력 층에도 편향 도입 가능.    
   <br>
   
  ### ■ 시행착오 전략
   퍼셉트론은 실수를 통해 배우는 시행착오 전략으로 학습.    
   손실 함수 값을 최소가 되게 하도록 가중치를 조절함.   
   입력의 가중 한 계산, 활성화 함수에 입력 → 예측값 결정(순방향 계산) → 예측 값과 실제 레이블 값을 비교, 오차 계산 → 오차에 따라 가중치 조정, 오차가 0에 가깝도록 과정 반복   
    <br>
    
## 2-2. 다층 퍼셉트론(Multilayer Perceptron, MLP)
 퍼셉트론은 직선 함수기 때문에, 선형 데이터는 분류할 수 있지만 하나의 뉴런만으로 비선형 데이터를 처리하기 어려움.   
 비선형 데이터를 다루기 위해 적어도 2개 이상의 직선 필요.  
 은닉층이 여러 층으로 구성. 층끼리는 서로 가중치 결합을 통해 연결되어 있음.    
  <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-10.png"></img><br>
    
 #### 다층 퍼셉트론의 구조
 - 입력층    
 - 은닉층: 실제로 데이터가 학습되는 곳. 복잡한 데이터를 다룰 수록 은닉층을 늘림. 뒤쪽으로 갈수록 복잡한 특징을 학습    
 - 결합 가중치: 노드 간 연결에 입력에 영향력을 나타내는 가중치가 부여     
 - 출력층: 뉴런에서 사용하는 활성화 함수의 종류에 따라 결정    
 </br></br>
 
 #### 신경망 설계
  신경망의 층수와 각 층을 이루는 노드 수를 결정. 층수가 많을수록 계산 비용이 많이 소요.     
  다른 사람의 구현에 성공한 신경망 설계(baseline)을 참고해 출발점을 잡는게 좋음(튜닝).     
  심층 신경망 = 은닉층이 2개 이상인 신경망    
  신경망의 층수가 많을 수록 복잡한 학습 데이터에 부합. 은닉층을 무작정 늘리면 overfitting 발생 가능, 새로운 데이터에 대한 예측이 오히려 부정확해짐.    
  </br>
  
 #### 전결합층(Fully Connected layer, FC)
  고전적 다층 퍼셉트론에서 특히 중요시 되는 구조. 이전 층의 모든 노드가 각기 다른 층의 모든 노드와 연결 됨    
 <br>
 
 #### 신경망 하이퍼파라미터 관련 중요 사항
 1. 은닉층 수    
  작은 규모의 신경망으로 시작해서 성능에 따라 차츰 층을 늘려가는 게 좋음. 학습 데이터와 테스트 데이터의 정확도가 모두 높아지는 지점까지만 학습하는게 좋음. (과적합 예방)      
 2. 활성화 함수    
  ReLU, Softmax 함수가 가장 널리 사용되며, ReLU는 출력층에, Softmax는 출력층에 사용하는 것이 좋음.     
 3. 오차 함수    
  회귀 문제에서는 평균제곱오차(Mean Square Error)가 많이 사용됨.    
  분류 문제에서는 교차 엔트로피(Cross Entropy)가 많이 사용.    
 4. 최적화 기법
  배치 경사 하강법, 확률적 경사 하강법, 미니배치 경사 하강법, **Adam, RMSprop** 등이 많이 사용됨. 한 번씩 써보고 가장 좋은 알고리즘을 찾음.   
 6. 배치 크기    
  파라미터를 한번 업데이트 할 때 마다 신경망에 입력되는 학습 데이터의 수를 말하며, 미니배치 경사 하강법에 큰 영향을 줌.   
  배치 크기가 클수록 학습 시간이 빨라지나, 메모리 용량이 많이 필요해짐.
  기본값 32로 시작해서 64, 128, 256으로 늘리는 것을 추천      
 7. 에포크 수   
  테스트 데이터에 대한 정확도가 하락하기 시작할 때 까지 에포크 수를 조금씩 늘려가는 것이 좋음    
 8. 학습률
  최적화 알고리즘의 파라미터로, 이론상 학습률이 충분히 작아야 최적 파라미터에 도달할 수 있으나, 무조건 작으면 도달하기까지가 오래 걸릴 수 있음.    
  일반적으로 딥러닝 라이브러리에 지정된 기본값부터 출발하는 것이 적당하며, 자릿수를 하나씩 올리거나 내리는 식으로 조절하며 변경하는 것이 좋음    
 </br></br>
 
 ## 2-3. 활성화 함수
 선형 모델을 아무리 여러개 놓고 학습 해봤자 결과는 선형 모델임. 선형 결합을 비선형 모델로 만들어 주는 과정이 필요한데, 이 역할을 해주는 게 활성화 함수임.   
 출력값을 특정 구간 안으로 제약하는 효과도 있음.    
 전이 함수, 비선형성이라고도 함.    
 </br>
 #### 활성화 함수 종류
 1. 선형 전달 함수(Linear transfer function)
  입력을 그대로 출력하는 함수. 가중 평균의 배율을 조정하는 효과는 있으나, 활성화 함수의 역할을 하지 목함.    
  선형 함수의 도함수는 상수기 때문에 입력과 아무 상관이 없고, 역전파 기울기도 항상 동일한 값이 되서 오차를 줄일 수 없기 때문에, 좋은 학습 모델이 아님.       
 </br>
 
 2. 스텝 함수(Step Function)
  이진 분류 문제에 주로 쓰이며, 출력값은 0,1임.      
  x > 0 이면 1 출력     
  x < 0 이면 0 출력     
  </br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-14.png"></img></br>
  
 3. 시그모이드 함수/로지스틱 함수      
  모든 입력값을 특정 구간(0~1)로 바꿔주는 함수. 때문에 극단적인 출력 값이나 예외 출력값이 존재하지 않음.    
  함수 그래프가 매끄러운 S자 모양 곡선     
  주로 이진 분류에서 두 클래스의 확률을 구할 때 사용.     
  <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-15.png"></img><br>
  
  4. 소프트맥스 함수     
   시그모이드 함수의 일반형. 즉, 3개 이상의 클래스에 대해 확률을 구할 때 사용.     
   출력 값은 0에서 1사이의 값이고, 각 클래스 확률의 총 합은 항상 1임. 주로 다중 분류 문제에서 사용.     
   <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-16.png"></img><br>
   
  5. 하이퍼볼릭 탄젠트 함수
    시그모이드 함수를 y축 이동한 함수로, -1부터 1 사이의 값을 출력.    
    데이터의 평균이 0.5(0~1사이 값)인 시그모이드와 달리 tanh 함수는 평균이 0에 가까워지기 때문에 데이터를 중앙에 모으는 효과가 있어 다음층의 학습에 더 유리함.  
    시그모이드, 탄젠트 함수 모두 입력 값이 매우 크거나 작을 경우, 함수의 기울기가 매우 작아지기 때문에 경사 하강법을 이용한 학습이 느려지게 함. ReLU 활성화 함수를 이용해 문제 해결 가능.     
    <br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_2-17.png"></img><br>
    
  6. 정류 선형 유닛
 
    
    
    
    
  
  
    
  
