## 4장. 딥러닝 프로젝트 시동걸기와 하이퍼파리미터 튜닝
</br>
강의: 세종대학교 최유경교수님 딥러닝시스템 강의(2023)    
</br></br>

## 목차
  1. 성능지표
  2. 베이스라인 모델 설정하기
  3. 학습 데이터 준비하기
  4. 모델을 평가하고 성능 지표 해석하기
  5. 신경망을 개선하고 하이퍼파라미터 튜닝하기
  6. 학습 및 최적화
  7. 최적화 알고리즘
  8. 과적합을 방지하기 위한 규제화 기법
  9. 배치 정규화
</br></br></br>

## 4-1. 성능지표
  모델의 성능을 평가할 수 있는 지표로, 가장 간단한건 정확도임.(정답을 맞힌 횟수/전체 표본 수)     
  정확도가 항상 적절한 지표는 아니며, 문제마다 적절한 평가 지표가 존재함.
  예) 백만명 중 1명 꼴로 발병하는 희귀질환 유무 진단 모델에서, 정확도가 99%이어도 실제 질환을 가진 사람을 찾지 못할 수 있음.      
  모델 학습 시 나타날 수 있는 결과의 모든 경우의 수를 정리한 표를 **혼동행렬(Confusion matrix)**이라고 함. 위의 예시는 4가지 경우로 분류할 수 있음.     
  </br>

  1. 진양성: 모델이 양성임을 정확히 예측
  2. 진음성: 모델이 음성임을 정확히 예측
  3. 위양성: 실제는 음성이지만 모델이 양성이라고 잘못 예측
  4. 위음성: 실제는 양성이지만 모델이 음성이라고 잘못 예측
  </br>

  위의 분류 결과 중 가장 치명적인 건 위음성이므로, **재현율**로 모델을 평가하는게 맞음. 또 다른 예시로 스펨 메일 분류기의 경우에는, **정밀도**가 적합한 지표임.
  #### 재현율(Recall, 또는 민감도)
   질환이 있는 사람을 음성으로 진단한한 위음성이 얼마나 되는지를 나타냄      
  #### 정밀도(Precision, 또는 특이성)
   재현율의 반대 개념으로, 질환이 없는 사람을 양성으로 진단한 위양성이 얼마나 되는지를 나타냄
  </br>

  질환 판정 모델은 재현율이 더 중요한 지표지만, 정밀도 역시 함께 살펴봐야 함. 재현율(r)과 정밀도(p)를 단일 지표로 한꺼번에 나타내고 싶은 경우 F-점수를 사용함.    
  F-점수는 정밀도와 재현율의 조화평균으로 정의함.     
  </br></br></br>

## 4-2. 베이스라인 모델 설정하기
  문제 종류에 따라 신경망 구조에 맞는 베이스라인을 모델을 설정해야 함.
  만약 해결하려는 문제에 대한 연구가 많이 진행된 상태면, 기존 모델과 알고리즘을 답습하는게 좋음. 다른 데이터셋에 이미 학습된 모델을 사용하는 전이학습(Transfer Learning)도 좋음.     
  </br>

  #### 고려사항
  - 사용할 신경망 유형(MLP, CNN, RNN 등)
  - YOLO나 SSD 같은 물체 인식 기법 적용 여부
  - 신경망의 층 수
  - 활성화 함수 종류
  - 최적화 알고리즘 종류
  - 과적화 방지 필요성(드롭아웃, 배치 정규화 등의 규제화 기법)
  </br></br></br>

## 4-3. 학습 데이터 준비하기
### ■ 훈련 데이터, 검증 데이터, 테스트 데이터 분류
  학습 시작 전 기본적인 데이터 준비 기법으로, 머신러닝 모델 학습을 위해 학습 데이터를 크게 **훈련 데이터**, **테스트 데이터**로 분할.    
  </br>

  - 훈련 데이터: 실제 학습에 사용    
  - 테스트 데이터: 학습된 모델의 성능 평가에 사용. **절대 학습에 사용되면 안 됨**.         
  </br>

  각 에포크가 끝날 때 마다 모델의 성능을 체크하고, 하이퍼 파라미터를 튜닝하는데, 이와 같이 학습 중 파라미터 튜닝에 사용되는 데이터 셋을 **검증 데이터**라고 함.
  </br><img src=""></img></br>

  **데이터 셋 규모가 작은 경우** 훈련 데이터와 테스트 데이터의 비율로 **80:20** 또는 **70:30**을 많이 사용.    
  검증 데이터를 추가한다면 60:20:20 이나 70:15:15로 나눔.    
  **데이터 셋 규모가 크면** 테스트 데이터와 검증 데이터는 **전체의 1%**로 잡아도 충분.
  </br>

  데이터 분할 시 데이터들이 같은 데이터 분포를 따르는지 확인해야 함.       
  예) 훈련 데이터는 고품질 이미지인데, 테스트 데이터는 저품질 이미지인 경우     
  </br>

  학습에 사용할 데이터 셋의 상태나 문제 유형에 따라 적절한 전처리 기법을 사용하는 것도 중요함.
  </br>
  #### 전처리 기법 종류
  - 회색조 이미지 변환
  색상 정보가 불필요 하거나 학습 계산 복잡도를 경감시켜야 하는 경우 고려될만 함.    
  </br>

  - 이미지 크기 조절
  신경망 학습을 하려면 입력되는 모든 이미지 크기가 다 같아야 함. 때문에 상황이 따라 이미지 조절이 필요함.    
  </br>

  - 데이터 정규화
  데이터에 포함된 입력 특징의 배욜을 조정해(각 픽셀 값에서 픽셀값의 평균을 빼고 픽셀값의 표준편차로 나누기) 비슷한 분포를 갖게 함.  훈련 데이터와 테스트 데이터를 동일한 평균, 표준편차로 정규화 해야 함.      
  </br><img src=""></img></br>

## 4-4. 모델을 평가하고 성능 지표 해석하기
  모델 학습이 끝난 후 성능이 좋지 않으면 해당 원인이 무엇인지(과적합, 과소적합 등)해당 구성 요소를 살펴보고 확인하는 과정을 말함. 학습 오차와 검증 오차의 추이를 보여주는 그래프로 확인하는 게 좋음.   
  </br>
  1. 과적합: 훈련 데이터에 대한 성능은 높은데 검증 데이터에 대한 성능이 상대적으로 낮은 경우. 하이퍼파라미터 조정 필요.
  2. 과소적합: 훈련 데이터에 대한 성능이 낮은 경우. 은닉층을 추가, 에포크 수 늘리기, 다른 신경망 구조 사용 등으로 해결해야 함.
  </br><img src=""></img></br>
  </br></br>

## 4-5. 신경망을 개선하고 하이퍼파라미터 튜닝하기
  - 하이퍼파라미터: 신경망의 학습 데상이 아니며, 유저가 값을 지정할 수 있음.
  - 파라미터: 학습 과정을 통해 값이 조정됨.
  </br>
  
  #### 신경망의 하이퍼파라미터
  1. 신경망 구조   
   은닉층 수(깊이) / 각 층 뉴런수(폭) / 활성화 함수 종류     
  2. 학습 및 최적화   
   학습률과 학습률 감쇠 유형 / 미니배치 크기 / 최적화 알고리즘 종류 / 에포크 수(조기 종료 적용 여부 포함)     
  3. 규제화 및 과적합 방지 기법    
   L2 규제화 / 드롭아웃 층 / 데이터 강화
  </br>

  하이퍼파라미터 튜닝은 모든 상황에 유효한 값이 없기 때문에 까다로움.      
  </br></br>

  - **신경망의 깊이와 폭**   
   데이터 셋이 복잡할수록 신경망의 학습 능력이 많이 필요한데, 신경망의 깊이와 폭은 신경망의 학습 능력과 직결됨.      
   신경망의 규모가 작으면 과소적합, 규모가 너무 크면 과적합 발생.
  </br>

  - 과적합: 모델의 학습 능력이 지나치게 높음.(은닉층 수 또는 유닛 수가 과도하게 많은 경우). 은닉층의 유닛수를 감소시키거나 드롭아웃같은 적절한 규제를 적용해야 함.
  - 과소적합: 모델의 학습 능력이 부족(은닉층 수 또는 유닛 수가 부족한 경우). 은닉층의 유닛 수를 증가시켜야 함.
  </br>

  적당한 규모 가늠 위해서는 시작점(베이스라인)을 정하고, 성능 관찰을 통해 다음 규모를 가감하면서 조절해야 함.     
  </br></br>

  - **활성화 함수의 종류**   
   뉴런에 비선형성을 도입하는 수단으로, 연구가 활발히 진행되고 있음.     
   일반적으로 ReLU나 ReLU변형 모델 성능이 우수하다고 알려져 있음.     
  </br></br></br>

## 4-6. 학습 및 최적화
  ### ■ 학습률
  매우 중요한 하이퍼파라미터. 오차 함수의 경사를 얼마나 빨리 내려갈지 정함(보폭).   
  **학습률이 높으면 최적화 속도는 빨라지고, 성능은 낮아짐.**    
  </br><img src=""></img></br>
  #### 학습률 유형
  1. 매우 이상적인 학습률 설정 (거의 불가능)
   한 번에 최소점 도달함.    
  2. 학습률이 이상적 학습률 작거나 훨씬 작은
   시간이 걸리지만 결국 최소점에 도달함.    
  3. 학습률이 이상적 학습률보다 큼
   그럭저럭 괜찮은 지점에서 수렴할 수는 있으나, 우리가 원하는 최소점과 거리가 있음(또는 있을 수 있음)      
  4. 학습률이 이상적 학습률보다 훨씬 큼 (최악의 경우)
   가중치가 최소점을 지나쳐 오히려 원래보다 더 최소점에서 멀어지게 됨. (발산, NaN)     
  </br>
  모든 딥러닝 라이브러리에서 미리 설정된 학습률 기본값은 좋은 출발점이 됨. 각각의 최적화 알고리즘에도 미리 설정된 기본값이 있음.   
  </br>
  #### 검증데이터에 대한 손실 값 분석 방법
  - 파라미터 수정 될 때 마다 val_loss가 감소: 설정된 하이퍼파라미터값이 정상.    
  - 학습이 끝날 때 까지 val_loss가 계속 감소: 학습률이 너무 작음. 에포크 수를 늘리거나, 학습률을 증가시켜야 함.    
  - Val_loss가 증감을 반복하며 진동: 학습률이 너무 큼. 학습률을 감소 시켜야 함.    
  </br>
  </br>

  ### ■ 학습률 감쇠(Learning rate decay)
  학습 진행 도중 학습률을 변화시키는 방법. 고정된 학습률보다 성능이 뛰어나고, 학습 시간을 크게 줄일 수 있음.    
  학습 초기에는 학습률 값을 크게 설정했다가, 점차 학습률을 감소시켜 최소점을 지나치는 현상을 방지함. 종류는 계단형 감쇠, 지수형 감쇠, 적응형 학습이 있음.   
  </br>
  - **계단형 감쇠**      
   일정 비율로 학습률을 감소
  </br><img src=""></img></br>
  </br> 
  
  - **지수형 감쇠**    
   8 에포크 마다 학습률에 0.1씩 곱함. 계단형 감쇠에 비해 시간이 더 걸리지만, 결국 수렴에 다다를 수 있음.   
  </br><img src=""></img></br>
  </br>
  
  - **적응형 학습**       
   학습 진행이 멈추는 시점에서 설정된 값 만큼 학습률을 자동으로 수정(감소 또는 증가). 일반적인 학습률 감쇠 기법보다 높은 성능을 보이며, Adam과 Adagrad가 적응형 학습이 적용된 최적화 알고리즘임.      
  </br><img src=""></img></br>
  </br>

  ### ■ 미니배치 크기
  배치 크기는 필요한 리소스 요구 사항과 학습 속도에 큰 영향을 미침.    
  만약 사용 중인 데이터 셋 크기가 작다면, BGD를 사용해도 빠른 학습이 가능하지만, 데이터셋 크기가 큰 경우 **64, 혹은 128**로 배치 크기를 잡고 시작하는 게 좋음. 이후 적당한 학습 속도가 나올 때 까지 32, 64, 128, 256, 512, 1024와 같이 배치 크기를 2배씩 늘려감.    
  1024 이상 미니배치는 잘 사용되지 않으며, 사용중인 컴퓨터 메모리 용량을 고려하며 사이즈를 결정해야 함.    
  </br>

  #### 경사하강법 리뷰
  1. **배치경사하강법(BGD)**
   - 데이터 셋 전체를 신경망에 입력해 학습   
   - 최적화 알고리즘이 전체 훈련 데이터를 대상으로 오차 계산을 하므로, 가중치 수정이 한 에포크 당 한 번만 일어남    
   - 소규모 데이터셋 사용할 때 유리
   - 장점은 노이즈가 적고 최소점까지 큰 보폭으로 접근 가능하다는 것
   - 단점은 가중치 한 번 수정하려면 전체 데이터셋이 필요하기 때문에, 학습 속도가 느림. 또한 메모리 요구량도 증가해서 감당 못 할 수도 있음.   
  </br>

  2. **미니배치 경사하강법(MB-GD)**
   - 가중치 수정에 모든 데이터셋 하나를 사용하는 대신, 미니배치로 분할된 여러개의 훈련 데이터를 사용.   
   - 행렬곱을 이용한 계산 속도 향상   
   - 가중치 수정하는데 걸리는 시간 짧아짐.  
  </br></br></br>

## 4-7. 최적화 알고리즘
광범위한 문제에 일반적으로 적용할 수 있고, 효과적인 방법은 경사 하강법과, 경사 하강법에서 파생된 방법들이 효과적이라고 알려짐.   
</br>

  ### ■ 모멘텀을 적용한 경사하강법
  확률적 경사 하강법의 경우, 오차의 최소점으로 이동할 때 진동이 일어남. 이런 이동방향의 진동을 감소시키기 위해 **모멘텀(Momentum)**이 고안됨.   
  경사가 기존 이동 방향과 같으면 이동 폭을 증가시키고, 기존 이동 방향과 다른 방향의 경사에는 이동 폭을 감소시킴.   
  모멘텀을 사용하면 가중치의 진동을 완화시키고, 원하는 지점에 더 빨리 수렴 가능함.   
  </br><img src=""></img></br>

  #### Adam(Adaptive Moment Estimation, 적응형 모멘트 예측)
  모멘텀과 같이, 이전에 계산했던 경사의 평균을 속도항으로 사용함. 하지만 모멘트와 달리, 속도항이 지수적으로 감쇠됨. 다른 최적화 알고리즘보다 학습 시간이 빠르며, 하이퍼파라미터는 학습률만 조정하면 됨.    
  비유하자면,   
  모멘텀: 경사를 굴러 내려가는 공   
  Adam: 무거운 공이 마찰력을 가진 바닥을 굴러 내려감. 내려가면서 모멘트가 감소함.    
  </br>
  ### ■ 조기 종료
  에포크(Epoch)가 많을 수록, 즉 반복 학습 횟수가 많을 수록 신경망은 더 많은 특징을 학습할 수 있음. 반복 학습 횟수가 충분한지 확인하기 위해서는 학습 중 훈련데이터와 검증 데이터의 오차를 잘 관찰해야 함.    
  훈련 오차와 검증 오차가 같이 줄어들다 갑자기 검증오차가 증가하는 지점이 있다면, 해당 지점이 과적합이 일어난 부분임. 해당 부분에 도달하기 전에 아예 **학습을 종료**해버리는 기법을 **조기 종료**라고 함.    
  검증 오차가 증가하기 시작하면 학습을 중지하는 방식. 덕분에 최대 에포크 수 결정에 덜 신경을 써도 됨. 에포크 수를 크게 설정해놓고 학습 하면 적당한 시점에 학습을 종료해줌.    
  </br></br></br>

## 4-8. 과적합을 방지하기 위한 규제화 기법
학습 중인 신경망에 과적합 발생시, 신경망의 표현력을 감소시키는 방법임.
</br>

  **1. L2 규제화**
   오차함수에 규제화항을 추가하는 것. 
   </br><img src=""></img></br>
   규제화된 은닉층 유닛의 가중치를 0에 가깝게 감소시키기 때문에, 가중치 감쇠라고도 함.(모델 표현력이 감소)   
  </br>

  **2. 드롭아웃**
   학습이 반복될 때 마다 전체 뉴련중 정해진 비율(p)만큼의 뉴련을 해당 회차 동안 비활성화 시키는 방법.   
   p의 값은 0.3에서 0.5사이 값으로 설정함.(초기에는 0.3으로 시작했다가, 과적합이 그래도 발생하면 p값을 올려 대응함)   
   L2 규제화와의 차이점은, 가중치를 통해 뉴런의 영향력을 억제하는 L2규제화와 달리 드롭아웃은 뉴런의 영향력을 아예 막아버림.   
  </br>

  **3. 데이터 강화**          
   필요에 따라 기존 데이터의 약간의 변형(반전, 회전, 배율조정, 밝기 조절 등)을 가한 새로운 데이터를 만들고, 이를 학습 데이터에 추가함. 저렴한 비용으로 훈련 데이터의 양을 늘려 과적합을 방지할 수 있음.    
   데이터 강화는 신경망이 새로운 데이터를 학습하더라도 유연하게 대응할 수 있는 능력을 길러줌. 원래 모습에 대한 의존도를 낮춰주기 때문에, 규제화 기법으로 취급되기도 함.    
  </br></br></br>

## 4.9 배치 정규화(Batch Nomalization, BN)
   공변량 시프트를 완화하기 위한 대책으로 등장. 학습 데이터를 전처리 하는 게 아니고, 추출된 특징을 정규화 하는 방법. 추출된 특징들은 변화가 심하기 때문에, 정규화하면 신경망의 학습속도와 유연성을 개선할 수 있음.    
  </br>
  #### 공변량 시프트 문제란?
  </br><img src=""></img></br>
  데이터셋 X를 레이블 Y에 매핑되도록 모델 학습 후, X의 분포에 변화가 일어난 경우, 공변량 시프트가 발생했다고 함.(분산과 평균의 변화)    
  아래 그림에서 L1 파라미터가 변화하면 L2의 입력도 변화하고, L3 입장에서 L2의 출력은 항상 변화됨. 즉, 해당 신경망에 공변량 시프트 문제가 발생하고 있음.    
  이러한 은닉층 출력의 분포 변화를 억제하기 위해 배치 정규화를 사용함.(은닉층 유닛의 출력이 항상 표준 분포를 따르도록 강제)   
  배치 정규화 기법을 적용하면 앞층의 출력이 항상 같은 평균과 분산을 갖게 되므로, 앞층 학습 결과의 영향을 덜 받게됨.(독립적인 학습 가능, 뒷층의 학습이 쉬워짐)
  </br><img src=""></img></br>
  각 층의 활성화 함수 앞에서, 아래와 같은 과정의 연산을 추가함.   
  </br>

  1. 입력된 미니배치의 평균과 표준 편차를 계산, 입력의 평균을 0으로 조정   
  2. 평균이 0으로 조정된 입력을 정규화
  3. 연산 결과의 배율 및 위치 조정. 두 파라미터를 찾는데 학습 시간이 좀 걸리지만, 찾고 나면 빨라짐.
  </br>
