# 6장. 전이학습
</br>
강의: 세종대학교 최유경교수님 딥러닝시스템 강의(2023)     
</br></br>

## 목차
1. 전이학습이란
2. 전이학습의 원리
3. 전이학습의 세 가지 방식
4. 적합한 전이학습 수준 선택하기
5. 오픈소스 데이터셋
</br></br></br>

## 6-1. 전이학습이란
전이학습이란, 신경망이 어떤 task를 위해 많은 양의 데이터를 이용해 학습한 지식(특징 맵)을 학습 데이터가 상대적으로 적은 다른 task로 옮겨오는 것을 말함. 즉, 학습된 신경망을 활용해 새로운 학습 데이터를 학습시키는 과정임.          
예) 개, 고양이 구분 모델을 구현할 때, 이미지넷 학습이 된 VGG16모델을 활용    
주로 구현된 신경망과 가중치를 같이 내려받는데, 모델을 새로 리모델링 하고 싶다면 가중치만 받아서 써도 됨.     
Keras의 경우, 학습된 신경망의 분류기 부분의 가중치를 제외하고 받거나, 추출 부분에 해당하는 층의 가중치를 고정시킬수도 있음.     

</br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_6-2.png"></img></br>
</br></br></br>

## 6-2. 전이학습의 원리
학습된 신경망은, 이미 가중치 학습과 하이퍼파라미터 튜닝을 거쳐 만족스러운 성능을 가졌음. 충분한 데이터로 학습되어진 신경망은 모서리, 꼭짓점 등 저수준 특징부터 원, 사각형, 바퀴같은 중수준 또는 고수준의 다양한 특징을 학습한 상태기 때문에, 아예 처음부터 시작하는 것보다 더 빠른 학습과 높은 성능을 얻을 수 있음.     
</br>

#### +) 저수준, 고수준 특징
- 저수준 특징: 거의 대부분의 다른 task에서도 활용할 수 있는 일반적인 정보
- 고수준 특징: 특정 task에 특화되어 다른 task에서 재사용하기 어려움   
</br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_6-7.png"></img></br>
</br></br></br>

## 6-3. 전이학습의 세 가지 방식
#### 1. 사전 학습된 신경망을 분류기로 이용
  **학습된 신경망을 그대로 받아서 사용**하는 방법. **원 도메인과 목표 도메인이 매우 유사**하고, 사전 학습된 신경망을 즉시 사용할 수 있는 경우에 적합.   
</br>

  예) 견종을 구분하는 모델을 만들때, 이미지넷 데이터에는 다양한 견종의 이미지가 포함되어 있기 때문에 이를 학습한 VGG16모델을 그대로 사용.    
</br>

#### 2. 사전 학습된 신경망을 특징 추출기로 이용    
 학습된 신경망의 **특징 추출기 부분의 가중치는 고정**, **분류기 부분을 제거**한 후 **새로운 분류기 부분을 추가**해서 사용.   
 원 도메인과 목표 도메인이 큰 차이가 없을 때 사용함. 그러나 사전 학습된 신경망은 원 분류 task에 특화된 부분이 많기 때문에, 효율적인 학습을 위해서 원래 분류기를 제거 후 새로 분류기를 추가하는 것이 좋음.    
 </br>
 
  예) 이미지넷 학습한 VGG16을 사용, 개/고양이 분류기 모델 설계    
 이미지넷은 1000개 이상의 분류 클래스가 있기 때문에, 이대로 쓰기보다는 개와 고양이만을 구분하는 분류기를 만들어 쓰는게 훨씬 효율적임.     
</br> 

#### 3. 미세 조정(Fine tuning)     
 목표 도메인과 원 도메인이 많이 동떨어진 경우(또는 전혀 다른 경우) 사용됨. 특징 추출에 쓰이는 신경망의 일부 층을 고정하고, 고정하지 않은 층과 새로 추가된 층을 다같이 학습하는 방식임. 특징 추출기 부분을 재학습 하면서 고수준 특징이 새로운 task에 적합하게 조정되서 미세 조정이라고 함.    
</br><img src="https://drek4537l1klr.cloudfront.net/elgendy/HighResolutionFigures/figure_6-10.png"></img></br>
</br>

#### 미세 조정의 장점

처음부터 학습시킨다면 가중치가 무작위 값으로 초기화하고, 학습을 통해 최적의 가중치를 찾아가게 되는데, 이게 우리가 원하는 최적값과 가까이 있을 거라는 보장이 없음. 또한 초기값이 최적값과 거리가 멀면 가중치 수렴까지 시간이 많이 걸릴 수 있음.    
반면, 미세 조정을 할 경우 미리 학습된 신경망의 최적의 가중치를 사용해 학습할 수 있고, 가중치 수렴을 빨리 할 수 있음. 때문에 스크래치 단계에서 학습하는 것 보다 학습 속도가 빠름.     
</br>

